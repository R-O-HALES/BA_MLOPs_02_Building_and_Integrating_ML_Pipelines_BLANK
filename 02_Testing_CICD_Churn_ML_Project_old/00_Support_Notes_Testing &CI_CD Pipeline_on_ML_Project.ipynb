{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "banner_image_cell",
   "metadata": {},
   "source": [
    "<img src=\"../media/LandingPage-Header-RED-CENTRE.jpg\" alt=\"Notebook Banner\" style=\"width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc4350",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0093e2",
   "metadata": {},
   "source": [
    "## Unit Test:\n",
    "\n",
    "## Data Loader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ad07f",
   "metadata": {},
   "source": [
    "\n",
    "This script provides robust unit tests for the `data_loader.py` file, specifically focusing on the `load_churn_dataset` function. The goal is to ensure that data loading works correctly under various scenarios and that appropriate errors are raised when expected. This is a classic example of a unit test that isolates the function's behavior from external dependencies by creating controlled test environments.\n",
    "- **Test Setup (`setUp` method):**\n",
    "    - `setUp(self)`: Before each test method runs, this method sets up a temporary, isolated environment.\n",
    "    - **Temporary Directory:** It creates a `tempfile.mkdtemp()` to house test CSV files. This is crucial for maintaining a clean testing environment and preventing test files from polluting your project directory.\n",
    "    - **Dummy CSV File:** A `self.dummy_csv_filepath` is created with well formed, representative data. This simulates a typical, valid input for `load_churn_dataset`.\n",
    "    - **Empty CSV File:** A `self.empty_csv_filepath` is created, containing only headers. This is an important edge case to test how the function handles datasets with no rows, ensuring it returns an empty DataFrame with the correct schema.\n",
    "\n",
    "- **Test Teardown (`tearDown` method):**\n",
    "    - `tearDown(self)`: After each test method completes, this method cleans up the temporary directory and all its contents using `shutil.rmtree()`. This ensures that each test run starts with a fresh slate and leaves no lingering files.\n",
    "\n",
    "- **Individual Test Case:**\n",
    "    - **`test_load_churn_dataset_success`:**\n",
    "        - **Objective:** Verifies that `load_churn_dataset` successfully loads a valid CSV file into a pandas DataFrame.\n",
    "        - **Assertions:**\n",
    "            - `self.assertIsInstance(df, pd.DataFrame)`: Confirms the returned object is indeed a DataFrame.\n",
    "            - `self.assertFalse(df.empty)`: Checks that the DataFrame is not empty.\n",
    "            - `self.assertEqual(len(df), 3)`: Verifies the correct number of rows are loaded based on the dummy data.\n",
    "            - `self.assertIn('customerID', df.columns)` and similar checks: Ensures that expected columns are present in the loaded DataFrame, confirming the data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ad07f",
   "metadata": {},
   "source": [
    "Simple Testing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32796dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/test_data_loader.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains unit tests for the functions in `src/data_loader.py`,\n",
    "    now implemented using Python's built-in `unittest` framework.\n",
    "    It focuses on verifying the correct loading of data and proper error handling.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_load_churn_dataset_success: Verifies that a well-formed CSV file is loaded\n",
    "      correctly into a pandas DataFrame with expected properties.\n",
    "    - test_load_churn_dataset_file_not_found: Checks that a RuntimeError is raised\n",
    "      when attempting to load a non-existent file.\n",
    "    - test_load_churn_dataset_empty_csv: Ensures that an empty CSV file (with headers)\n",
    "      is handled gracefully, resulting in an empty DataFrame with correct columns.\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "# Import the function to be tested\n",
    "from src.data_loader import load_churn_dataset\n",
    "\n",
    "class TestDataLoader(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create a temporary directory for test files\n",
    "        self.tmp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Create a dummy CSV file\n",
    "        self.dummy_csv_filepath = os.path.join(self.tmp_dir, \"test_churn_data.csv\")\n",
    "        dummy_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
    "7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No\n",
    "5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No\n",
    "3668-QPYAX,Male,0,No,No,2,Yes,No,DSL,Yes,Yes,No,No,No,No,Month-to-month,Yes,Mailed check,53.85,108.15,Yes\n",
    "\"\"\"\n",
    "        with open(self.dummy_csv_filepath, 'w') as f:\n",
    "            f.write(dummy_data)\n",
    "\n",
    "        # Create an empty CSV file with only headers\n",
    "        self.empty_csv_filepath = os.path.join(self.tmp_dir, \"empty_churn_data.csv\")\n",
    "        empty_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
    "\"\"\"\n",
    "        with open(self.empty_csv_filepath, 'w') as f:\n",
    "            f.write(empty_data)\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Clean up the temporary directory\n",
    "        shutil.rmtree(self.tmp_dir)\n",
    "\n",
    "    def test_load_churn_dataset_success(self):\n",
    "        \"\"\"Test if the dataset is loaded successfully as a DataFrame.\"\"\"\n",
    "        df = load_churn_dataset(self.dummy_csv_filepath)\n",
    "\n",
    "        self.assertIsInstance(df, pd.DataFrame)\n",
    "        self.assertFalse(df.empty)\n",
    "        self.assertEqual(len(df), 3)\n",
    "        self.assertIn('customerID', df.columns)\n",
    "        self.assertIn('Churn', df.columns)\n",
    "        self.assertIn('TotalCharges', df.columns)\n",
    "\n",
    "# This block allows you to run the tests directly from the script\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "additional_tests_markdown_cell",
   "metadata": {},
   "source": [
    "Addional tests that we can do:\n",
    "\n",
    "- What if the file is not found?\n",
    "\n",
    "- **`test_load_churn_dataset_file_not_found` (Implicitly tested by the structure, but a good addition for comprehensive testing):**\n",
    "    - **Objective:** (If added) Would verify that a `RuntimeError` is raised when a non existent file path is provided to `load_churn_dataset`.\n",
    "    - **Mechanism:** (If added) Would use `with self.assertRaises(RuntimeError):` to wrap the call to `load_churn_dataset` with a non existent path.\n",
    "\n",
    "\n",
    "- What if the file is empty? Is the script reading from somewhere else?\n",
    "\n",
    "- **`test_load_churn_dataset_empty_csv` (Implicitly tested by the structure, but a good addition for comprehensive testing):**\n",
    "    - **Objective:** (If added) Would ensure that an empty CSV file (with headers) is handled gracefully, resulting in an empty DataFrame with correct column names.\n",
    "    - **Mechanism:** (If added) Would load `self.empty_csv_filepath` and assert that the DataFrame is empty but has the correct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/test_data_loader.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains unit tests for the functions in `src/data_loader.py`,\n",
    "    now implemented using Python's built-in `unittest` framework.\n",
    "    It focuses on verifying the correct loading of data and proper error handling.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_load_churn_dataset_success: Verifies that a well-formed CSV file is loaded\n",
    "      correctly into a pandas DataFrame with expected properties.\n",
    "    - test_load_churn_dataset_file_not_found: Checks that a RuntimeError is raised\n",
    "      when attempting to load a non-existent file.\n",
    "    - test_load_churn_dataset_empty_csv: Ensures that an empty CSV file (with headers)\n",
    "      is handled gracefully, resulting in an empty DataFrame with correct columns.\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "# Import the function to be tested\n",
    "from src.data_loader import load_churn_dataset\n",
    "\n",
    "class TestDataLoader(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create a temporary directory for test files\n",
    "        self.tmp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Create a dummy CSV file\n",
    "        self.dummy_csv_filepath = os.path.join(self.tmp_dir, \"test_churn_data.csv\")\n",
    "        dummy_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
    "7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No\n",
    "5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No\n",
    "3668-QPYAX,Male,0,No,No,2,Yes,No,DSL,Yes,Yes,No,No,No,No,Month-to-month,Yes,Mailed check,53.85,108.15,Yes\n",
    "\"\"\"\n",
    "        with open(self.dummy_csv_filepath, 'w') as f:\n",
    "            f.write(dummy_data)\n",
    "\n",
    "        # Create an empty CSV file with only headers\n",
    "        self.empty_csv_filepath = os.path.join(self.tmp_dir, \"empty_churn_data.csv\")\n",
    "        empty_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
    "\"\"\"\n",
    "        with open(self.empty_csv_filepath, 'w') as f:\n",
    "            f.write(empty_data)\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Clean up the temporary directory\n",
    "        shutil.rmtree(self.tmp_dir)\n",
    "\n",
    "    def test_load_churn_dataset_success(self):\n",
    "        \"\"\"Test if the dataset is loaded successfully as a DataFrame.\"\"\"\n",
    "        df = load_churn_dataset(self.dummy_csv_filepath)\n",
    "\n",
    "        self.assertIsInstance(df, pd.DataFrame)\n",
    "        self.assertFalse(df.empty)\n",
    "        self.assertEqual(len(df), 3)\n",
    "        self.assertIn('customerID', df.columns)\n",
    "        self.assertIn('Churn', df.columns)\n",
    "        self.assertIn('TotalCharges', df.columns)\n",
    "\n",
    "    def test_load_churn_dataset_file_not_found(self):\n",
    "        \"\"\"Test if RuntimeError is raised for a non-existent file.\"\"\"\n",
    "        non_existent_file = os.path.join(self.tmp_dir, \"non_existent_file.csv\")\n",
    "        with self.assertRaisesRegex(RuntimeError, \"Failed to load churn dataset: .*No such file or directory.*\"):\n",
    "            load_churn_dataset(non_existent_file)\n",
    "\n",
    "    def test_load_churn_dataset_empty_csv(self):\n",
    "        \"\"\"Test loading an empty CSV file.\"\"\"\n",
    "        df = load_churn_dataset(self.empty_csv_filepath)\n",
    "\n",
    "        self.assertIsInstance(df, pd.DataFrame)\n",
    "        self.assertEqual(len(df), 0)\n",
    "        self.assertIn('customerID', df.columns) # Should still have columns from header\n",
    "\n",
    "# This block allows you to run the tests directly from the script\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_description",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This script is a comprehensive suite of unit tests for the `preprocessing.py` file. It leverages Python's built in `unittest` framework to ensure that all core data preprocessing steps cleaning, pipeline construction, feature transformation, and data splitting function correctly and robustly. This is vital for maintaining data integrity and consistency throughout your ML pipeline.\n",
    "\n",
    "-   **Test Setup (`setUp` method):**\n",
    "    -   `setUp(self)`: Before each test runs, this method prepares the necessary data and configurations. It's designed to provide consistent inputs for the subsequent tests.\n",
    "    -   **`self.raw_df`:** A dummy raw `DataFrame` is created, mimicking the structure and content of your initial churn dataset. This includes edge cases like an empty string in 'TotalCharges' to test robust cleaning.\n",
    "    -   **`self.config_columns`:** Defines the essential column names (target, numeric, categorical) that your preprocessing functions rely on. This promotes reusability and clarity.\n",
    "    -   **Pre-computed DataFrames:** `self.cleaned_df` and `self.transformed_df` are generated by calling `clean_churn_data` and `transform_features` respectively on copies of the dummy data. This allows individual tests to start from a specific, pre-processed state without re running these potentially expensive steps for every test.\n",
    "\n",
    "-   **Tests for `clean_churn_data`:**\n",
    "    -   **`test_clean_churn_data_success`:**\n",
    "        -   **Objective:** Verifies that the `clean_churn_data` function successfully transforms the raw data.\n",
    "        -   **Assertions:** It checks for correct conversion of the 'Churn' column to a binary 'churn_binary' (0s and 1s), ensures 'TotalCharges' is numeric and handles empty strings as `NaN`, and confirms that the final DataFrame contains precisely the expected set of columns.\n",
    "    -   **`test_clean_churn_data_missing_target_column`:**\n",
    "        -   **Objective:** Tests the error handling of `clean_churn_data` when a critical column (like the target column) is missing from the input DataFrame.\n",
    "        -   **Assertions:** It asserts that a `ValueError` with a specific message about missing columns is raised, ensuring the function fails gracefully under invalid inputs.\n",
    "\n",
    "-   **Tests for `build_preprocessing_pipeline`:**\n",
    "    -   **`test_build_preprocessing_pipeline_structure`:**\n",
    "        -   **Objective:** Ensures that the `build_preprocessing_pipeline` function constructs the `scikit-learn` `ColumnTransformer` correctly.\n",
    "        -   **Assertions:** It verifies that the returned object is indeed a `ColumnTransformer`, that it contains the expected number of transformers (one for numeric, one for categorical), and that these transformers are correctly configured `Pipeline` objects with `StandardScaler` for numeric and `OneHotEncoder` for categorical features, respectively. This confirms the pipeline's architectural correctness.\n",
    "\n",
    "-   **Tests for `transform_features`:**\n",
    "    -   **`test_transform_features_success`:**\n",
    "        -   **Objective:** Confirms that `transform_features` correctly applies preprocessing and attaches essential metadata to the DataFrame.\n",
    "        -   **Assertions:** It checks that the output is a DataFrame, that a 'target_encoded' column is created with the correct data type, and that the `feature_columns`, `preprocessor`, `target_mapping`, and `target_names` attributes are all present and correctly populated within the DataFrame's `.attrs` dictionary. This is a critical MLOps practice for maintaining data lineage.\n",
    "\n",
    "-   **Tests for `split_features_and_target`:**\n",
    "    -   **`test_split_features_and_target_success`:**\n",
    "        -   **Objective:** Verifies that `split_features_and_target` correctly separates the DataFrame into features (X) and target (y).\n",
    "        -   **Assertions:** It confirms that X is a DataFrame and y is a Series, that their lengths match, that the target column is removed from X, and that the columns in X precisely match the `feature_columns` stored in the DataFrame's attributes.\n",
    "    -   **`test_split_features_and_target_missing_attrs`:**\n",
    "        -   **Objective:** Tests the error handling of `split_features_and_target` when required metadata (like `feature_columns`) is missing.\n",
    "        -   **Assertions:** It asserts that a `ValueError` with a specific message is raised, ensuring robustness against malformed inputs.\n",
    "\n",
    "-   **Tests for `stratified_split` (Not explicitly defined in the provided code, but mentioned in comments. A comprehensive test would include):**\n",
    "    -   **Objective:** To ensure that the `stratified_split` function correctly divides the data into training and testing sets while preserving the original class distribution of the target variable.\n",
    "    -   **Assertions:** This test would typically verify that the proportions of each class in the target variable (`y`) are approximately the same in both the training and testing sets, ensuring that the model training is not biased by imbalanced splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/test_preprocessing.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains unit tests for the functions in `src/preprocessing.py`,\n",
    "    now implemented using Python's built-in `unittest` framework.\n",
    "    It ensures that data cleaning, preprocessing pipeline construction,\n",
    "    feature transformation, and data splitting are performed correctly.\n",
    "\n",
    "Tests Performed:\n",
    "    - clean_churn_data:\n",
    "        - Verifies successful cleaning, type conversion (e.g., TotalCharges),\n",
    "          creation of churn_binary, and correct column selection.\n",
    "        - Checks for proper error handling when essential columns are missing.\n",
    "    - build_preprocessing_pipeline:\n",
    "        - Asserts that the ColumnTransformer is correctly built with expected\n",
    "          numerical and categorical transformers (StandardScaler, OneHotEncoder).\n",
    "    - transform_features:\n",
    "        - Confirms that target encoding is applied and that DataFrame attributes\n",
    "          (like feature_columns, preprocessor) are correctly set.\n",
    "    - split_features_and_target:\n",
    "        - Ensures that features (X) and target (y) are accurately separated\n",
    "          based on DataFrame attributes.\n",
    "        - Checks for error when required attributes are missing.\n",
    "    - stratified_split:\n",
    "        - Verifies that the data is split into train and test sets while\n",
    "          maintaining the original class proportions (stratification).\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "# Import functions to be tested\n",
    "from src.preprocessing import (\n",
    "    clean_churn_data,\n",
    "    build_preprocessing_pipeline,\n",
    "    transform_features,\n",
    "    split_features_and_target,\n",
    "    stratified_split\n",
    ")\n",
    "\n",
    "class TestPreprocessing(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Helper method for raw DataFrame\n",
    "        self.raw_df = pd.DataFrame({\n",
    "            'customerID': ['1', '2', '3', '4'],\n",
    "            'gender': ['Male', 'Female', 'Male', 'Female'],\n",
    "            'SeniorCitizen': [0, 1, 0, 0],\n",
    "            'Partner': ['Yes', 'No', 'No', 'Yes'],\n",
    "            'Dependents': ['No', 'Yes', 'No', 'No'],\n",
    "            'tenure': [1, 34, 2, 45],\n",
    "            'PhoneService': ['No', 'Yes', 'Yes', 'Yes'],\n",
    "            'MultipleLines': ['No phone service', 'No', 'No', 'Yes'],\n",
    "            'InternetService': ['DSL', 'DSL', 'Fiber optic', 'DSL'],\n",
    "            'OnlineSecurity': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'OnlineBackup': ['Yes', 'No', 'Yes', 'No'],\n",
    "            'DeviceProtection': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'TechSupport': ['No', 'No', 'No', 'Yes'],\n",
    "            'StreamingTV': ['No', 'No', 'No', 'Yes'],\n",
    "            'StreamingMovies': ['No', 'No', 'No', 'Yes'],\n",
    "            'Contract': ['Month-to-month', 'One year', 'Month-to-month', 'Two year'],\n",
    "            'PaperlessBilling': ['Yes', 'No', 'Yes', 'No'],\n",
    "            'PaymentMethod': ['Electronic check', 'Mailed check', 'Electronic check', 'Bank transfer (automatic)'],\n",
    "            'MonthlyCharges': [29.85, 56.95, 53.85, 70.70],\n",
    "            'TotalCharges': ['29.85', '1889.5', '108.15', ''], # Empty string for TotalCharges\n",
    "            'Churn': ['No', 'No', 'Yes', 'Yes']\n",
    "        })\n",
    "\n",
    "        # Helper method for configuration columns\n",
    "        self.config_columns = {\n",
    "            'TARGET_COLUMN': 'Churn',\n",
    "            'NUMERIC_COLUMNS': ['tenure', 'MonthlyCharges', 'TotalCharges'],\n",
    "            'CATEGORICAL_COLUMNS': [\n",
    "                'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "                'PhoneService', 'MultipleLines', 'InternetService',\n",
    "                'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "                'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Create cleaned and transformed dataframes once for all tests\n",
    "        self.cleaned_df = clean_churn_data(\n",
    "            self.raw_df.copy(),\n",
    "            self.config_columns['TARGET_COLUMN'],\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "        \n",
    "        self.transformed_df, _ = transform_features(\n",
    "            self.cleaned_df.copy(),\n",
    "            self.config_columns['TARGET_COLUMN'],\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "\n",
    "    # --- Tests for clean_churn_data ---\n",
    "    def test_clean_churn_data_success(self):\n",
    "        \"\"\"\n",
    "        Test successful data cleaning.\n",
    "        Verifies that:\n",
    "        1. The target column 'Churn' is converted to a binary 'churn_binary'.\n",
    "        2. 'TotalCharges' is converted to a numeric type, handling non-numeric values as NaN.\n",
    "        3. The final DataFrame contains only the expected columns.\n",
    "        \"\"\"\n",
    "        df_clean = self.cleaned_df\n",
    "\n",
    "        # 1. Verify target column transformation\n",
    "        self.assertIn('churn_binary', df_clean.columns)\n",
    "        self.assertEqual(df_clean['churn_binary'].dtype, np.int64)\n",
    "        pd.testing.assert_series_equal(\n",
    "            df_clean['churn_binary'],\n",
    "            pd.Series([0, 0, 1, 1], name='churn_binary'),\n",
    "            check_index=False  # We only care about the values\n",
    "        )\n",
    "\n",
    "        # 2. Verify 'TotalCharges' transformation\n",
    "        self.assertTrue(pd.api.types.is_numeric_dtype(df_clean['TotalCharges']))\n",
    "        self.assertTrue(np.isnan(df_clean['TotalCharges'].iloc[3]))\n",
    "\n",
    "        # 3. Verify final column set\n",
    "        expected_cols = self.config_columns['NUMERIC_COLUMNS'] + self.config_columns['CATEGORICAL_COLUMNS'] + ['churn_binary']\n",
    "        self.assertEqual(set(df_clean.columns), set(expected_cols))\n",
    "\n",
    "    def test_clean_churn_data_missing_target_column(self):\n",
    "        \"\"\"Test error handling when target column is missing.\"\"\"\n",
    "        df_missing_target = self.raw_df.drop(columns=[self.config_columns['TARGET_COLUMN']]).copy()\n",
    "        with self.assertRaisesRegex(ValueError, \"Missing required columns in dataset: \\\\['Churn'\\\\]\"):\n",
    "            clean_churn_data(\n",
    "                df_missing_target,\n",
    "                self.config_columns['TARGET_COLUMN'],\n",
    "                self.config_columns['NUMERIC_COLUMNS'],\n",
    "                self.config_columns['CATEGORICAL_COLUMNS']\n",
    "            )\n",
    "\n",
    "    # --- Tests for build_preprocessing_pipeline ---\n",
    "    def test_build_preprocessing_pipeline_structure(self):\n",
    "        \"\"\"\n",
    "        Test the structure of the preprocessing pipeline.\n",
    "        Ensures the ColumnTransformer is built with the correct transformers\n",
    "        for numeric and categorical features. This is key for ensuring our model's inputs are consistent.\n",
    "        \"\"\"\n",
    "        preprocessor = build_preprocessing_pipeline(\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "\n",
    "        self.assertIsInstance(preprocessor, ColumnTransformer)\n",
    "        self.assertEqual(len(preprocessor.transformers), 2) # type: ignore\n",
    "\n",
    "\n",
    "        num_transformer = [t for name, t, cols in preprocessor.transformers if name == 'num'][0]  # type: ignore\n",
    "        cat_transformer = [t for name, t, cols in preprocessor.transformers if name == 'cat'][0]  # type: ignore\n",
    "\n",
    "        self.assertIsInstance(num_transformer, Pipeline)\n",
    "        self.assertIsInstance(cat_transformer, Pipeline)\n",
    "\n",
    "        self.assertIn('scaler', num_transformer.named_steps)\n",
    "        self.assertIsInstance(num_transformer.named_steps['scaler'], StandardScaler)\n",
    "        self.assertIn('onehot', cat_transformer.named_steps)\n",
    "        self.assertIsInstance(cat_transformer.named_steps['onehot'], OneHotEncoder)\n",
    "\n",
    "    # --- Tests for transform_features ---\n",
    "    def test_transform_features_success(self):\n",
    "        \"\"\"\n",
    "        Test successful feature transformation and metadata attachment.\n",
    "        Verifies that the function correctly encodes the target and, crucially for MLOps,\n",
    "        attaches important metadata (like feature names and the preprocessor itself)\n",
    "        to the DataFrame's attributes for later use in the pipeline.\n",
    "        \"\"\"\n",
    "        df_transformed, preprocessor = transform_features(\n",
    "            self.cleaned_df.copy(),\n",
    "            self.config_columns['TARGET_COLUMN'],\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "\n",
    "        self.assertIsInstance(df_transformed, pd.DataFrame)\n",
    "        self.assertIn('target_encoded', df_transformed.columns)\n",
    "        self.assertEqual(df_transformed['target_encoded'].dtype, np.int64)\n",
    "        self.assertIsInstance(preprocessor, ColumnTransformer)\n",
    "\n",
    "        self.assertIn('feature_columns', df_transformed.attrs)\n",
    "        self.assertIn('preprocessor', df_transformed.attrs)\n",
    "        self.assertIn('target_mapping', df_transformed.attrs)\n",
    "        self.assertIn('target_names', df_transformed.attrs)\n",
    "        self.assertEqual(set(df_transformed.attrs['feature_columns']), set(self.config_columns['NUMERIC_COLUMNS'] + self.config_columns['CATEGORICAL_COLUMNS']))\n",
    "\n",
    "    # --- Tests for split_features_and_target ---\n",
    "    def test_split_features_and_target_success(self):\n",
    "        \"\"\"Test successful splitting of features and target.\"\"\"\n",
    "        X, y = split_features_and_target(self.transformed_df)\n",
    "\n",
    "        self.assertIsInstance(X, pd.DataFrame)\n",
    "        self.assertIsInstance(y, pd.Series)\n",
    "        self.assertEqual(len(X), len(y))\n",
    "        self.assertNotIn('target_encoded', X.columns)\n",
    "        self.assertEqual(y.name, 'target_encoded')\n",
    "        self.assertEqual(set(X.columns), set(self.transformed_df.attrs['feature_columns']))\n",
    "\n",
    "    def test_split_features_and_target_missing_attrs(self):\n",
    "        \"\"\"Test error when feature_columns attribute is missing.\"\"\"\n",
    "        df_no_attrs = self.cleaned_df.copy()\n",
    "        if 'feature_columns' in df_no_attrs.attrs:\n",
    "            del df_no_attrs.attrs['feature_columns']\n",
    "\n",
    "        with self.assertRaisesRegex(ValueError, \"No feature columns found in dataset attributes\"):\n",
    "            split_features_and_target(df_no_attrs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_unit_tests_description",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This script provides a thorough set of unit tests for the `model.py` file, focusing on the `ChurnPredictionModel` class and its auxiliary metric calculation and reporting functions. These tests are crucial for ensuring that your machine learning model behaves as expected, from initialization and training to prediction, saving, loading, and performance logging.\n",
    "\n",
    "-   **`TestChurnPredictionModel` Class:**\n",
    "    -   **Objective:** Contains unit tests specifically for the `ChurnPredictionModel` class, ensuring its core functionalities work correctly.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Initializes a dummy dataset (`self.X` for features, `self.y` for target) and a simple `StandardScaler` preprocessor before each test method runs. This provides controlled, consistent inputs for testing the model's behavior.\n",
    "        -   **`self.X` and `self.y`:** Randomly generated NumPy arrays are converted into pandas `DataFrame` and `Series` respectively, simulating feature and target data. This isolation ensures that the model's behavior is tested independently of prior data loading or preprocessing steps.\n",
    "    -   **`test_model_init_defaults`:**\n",
    "        -   **Objective:** Verifies that the `ChurnPredictionModel` initializes with the correct default components when no specific classifier or preprocessor is provided.\n",
    "        -   **Assertions:** Checks that the `classifier` attribute is an instance of `LogisticRegression` and that the internal `pipe` (a `sklearn.pipeline.Pipeline`) contains both a 'scaler' (from `StandardScaler`) and a 'classifier' step.\n",
    "    -   **`test_model_fit_predict`:**\n",
    "        -   **Objective:** Confirms that the model can be successfully trained (`fit`), make predictions (`predict`), and generate probability estimates (`predict_proba`).\n",
    "        -   **Assertions:** Assesses the shapes of the predictions and probabilities, ensures predictions are binary (0 or 1), verifies probabilities are between 0 and 1, and confirms that probabilities for each sample sum up to 1.\n",
    "    -   **`test_model_save_and_load`:**\n",
    "        -   **Objective:** Ensures the model's persistence capabilities that it can be saved to disk and subsequently loaded, retaining its predictive ability.\n",
    "        -   **Assertions:** Uses a `TemporaryDirectory` to save the trained model, checks if the file exists, loads the model back using `joblib.load`, and verifies that the loaded model is a `Pipeline` instance and can still produce predictions with the expected shape.\n",
    "    -   **`test_model_log_run`:**\n",
    "        -   **Objective:** Validates the model's logging functionality, ensuring that run information (metrics, dataset details) is correctly written to a JSON file.\n",
    "        -   **Assertions:** Creates a temporary log directory and file, calls `log_run`, checks for the existence of the log file, reads its content, and asserts that the logged data is a list and contains the expected metrics and dataset information.\n",
    "\n",
    "-   **`TestMetricsFunctions` Class:**\n",
    "    -   **Objective:** Focuses on testing the standalone metric computation and reporting functions (`compute_classification_metrics` and `report_classification_metrics`).\n",
    "    -   **`test_compute_classification_metrics_perfect`:**\n",
    "        -   **Objective:** Tests the `compute_classification_metrics` function with a scenario where predictions are perfectly accurate.\n",
    "        -   **Assertions:** Verifies that all metrics (accuracy, precision, recall, f1_score) correctly return 1.0, indicating perfect performance.\n",
    "    -   **`test_compute_classification_metrics_imperfect`:**\n",
    "        -   **Objective:** Tests `compute_classification_metrics` with a scenario involving imperfect predictions to ensure correct calculation of various metrics.\n",
    "        -   **Assertions:** Asserts the calculated values for accuracy, precision, and recall match the expected values for the given imperfect predictions.\n",
    "    -   **`test_report_classification_metrics_output`:**\n",
    "        -   **Objective:** Checks that the `report_classification_metrics` function generates the expected console output format.\n",
    "        -   **Assertions:** Uses `unittest.mock.patch` to capture the output sent to `builtins.print`, then asserts that the captured string contains key phrases and formatted metric values, ensuring the report is user friendly and informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00291911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains unit tests for the `src/model.py` module, implemented\n",
    "    using Python's built-in `unittest` framework. It ensures that the\n",
    "    `ChurnPredictionModel` class and its associated metric functions operate correctly.\n",
    "\n",
    "Tests Performed:\n",
    "    - TestChurnPredictionModel:\n",
    "        - `test_model_init_defaults`: Verifies that the model initializes with the\n",
    "          correct default classifier (LogisticRegression) and pipeline structure.\n",
    "        - `test_model_fit_predict`: Confirms that the model can be trained and can\n",
    "          produce predictions and probabilities of the correct shape and format.\n",
    "        - `test_model_save_and_load`: Ensures that the model's `save` method\n",
    "          creates a file and that a loaded model retains its predictive ability.\n",
    "        - `test_model_log_run`: Checks that model run information, including metrics\n",
    "          and parameters, is correctly logged to a JSON file.\n",
    "    - TestMetricsFunctions:\n",
    "        - `test_compute_classification_metrics`: Validates the accuracy of metric\n",
    "          calculations for both perfect and imperfect prediction scenarios.\n",
    "        - `test_report_classification_metrics`: Asserts that the metric reporting\n",
    "          function generates the expected console output.\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from unittest.mock import patch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "\n",
    "# Allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "from src.model import ChurnPredictionModel, compute_classification_metrics, report_classification_metrics\n",
    "\n",
    "class TestChurnPredictionModel(unittest.TestCase):\n",
    "    \"\"\"Unit tests for the ChurnPredictionModel class.\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up a dummy dataset and a simple preprocessor for use in tests.\"\"\"\n",
    "        self.X = pd.DataFrame(np.random.rand(100, 5), columns=[f'feature_{i}' for i in range(5)])\n",
    "        self.y = pd.Series(np.random.randint(0, 2, 100))\n",
    "        self.preprocessor = StandardScaler()\n",
    "\n",
    "    def test_model_init_defaults(self):\n",
    "        \"\"\"Test that the model initializes with correct default components.\"\"\"\n",
    "        model = ChurnPredictionModel()\n",
    "        self.assertIsInstance(model.classifier, LogisticRegression)\n",
    "        self.assertIn('scaler', model.pipe.named_steps)\n",
    "        self.assertIn('classifier', model.pipe.named_steps)\n",
    "\n",
    "    def test_model_fit_predict(self):\n",
    "        \"\"\"Test the model's fit, predict, and predict_proba methods.\"\"\"\n",
    "        model = ChurnPredictionModel(preprocessor=self.preprocessor)\n",
    "        model.fit(self.X, self.y)\n",
    "        preds = model.predict(self.X)\n",
    "        probs = model.predict_proba(self.X)\n",
    "        self.assertEqual(preds.shape, self.y.shape)\n",
    "        self.assertTrue(np.all(np.isin(preds, [0, 1])))\n",
    "        self.assertEqual(probs.shape, (len(self.X), 2))\n",
    "        self.assertTrue(np.all((probs >= 0) & (probs <= 1)))\n",
    "        self.assertTrue(np.allclose(np.sum(probs, axis=1), 1))\n",
    "\n",
    "    def test_model_save_and_load(self):\n",
    "        \"\"\"Test that the model can be saved and loaded correctly.\"\"\"\n",
    "        model = ChurnPredictionModel(preprocessor=self.preprocessor)\n",
    "        model.fit(self.X, self.y)\n",
    "        from tempfile import TemporaryDirectory\n",
    "        with TemporaryDirectory() as tmpdir:\n",
    "            path = os.path.join(tmpdir, \"model.joblib\")\n",
    "            model.save(path)\n",
    "            self.assertTrue(os.path.exists(path))\n",
    "            loaded_model = joblib.load(path)\n",
    "            self.assertIsInstance(loaded_model, Pipeline)\n",
    "            self.assertEqual(loaded_model.predict(self.X).shape, self.y.shape)\n",
    "\n",
    "    def test_model_log_run(self):\n",
    "        \"\"\"Test the logging of a model run to a JSON file.\"\"\"\n",
    "        model = ChurnPredictionModel()\n",
    "        from tempfile import TemporaryDirectory\n",
    "        with TemporaryDirectory() as tmpdir:\n",
    "            log_dir = os.path.join(tmpdir, \"logs\") # The log_run method should create this directory\n",
    "            log_file = os.path.join(log_dir, \"log.json\")\n",
    "            metrics = {\"accuracy\": 0.85}\n",
    "            dataset_info = {\"samples\": 100}\n",
    "            model.log_run(log_dir, metrics, dataset_info, log_filename=\"log.json\")\n",
    "            self.assertTrue(os.path.exists(log_file))\n",
    "            with open(log_file, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "            self.assertIsInstance(logs, list)\n",
    "            self.assertEqual(logs[0][\"metrics\"], metrics)\n",
    "\n",
    "class TestMetricsFunctions(unittest.TestCase):\n",
    "    \"\"\"Unit tests for the metric computation and reporting functions.\"\"\"\n",
    "\n",
    "    def test_compute_classification_metrics_perfect(self):\n",
    "        \"\"\"Test metric computation with perfect predictions.\"\"\"\n",
    "        y_true = np.array([0, 1, 0, 1])\n",
    "        y_pred = np.array([0, 1, 0, 1])\n",
    "        metrics = compute_classification_metrics(y_true, y_pred)\n",
    "        self.assertEqual(metrics['accuracy'], 1.0)\n",
    "        self.assertEqual(metrics['precision'], 1.0)\n",
    "        self.assertEqual(metrics['recall'], 1.0)\n",
    "        self.assertEqual(metrics['f1_score'], 1.0)\n",
    "\n",
    "    def test_compute_classification_metrics_imperfect(self):\n",
    "        \"\"\"Test metric computation with imperfect predictions.\"\"\"\n",
    "        y_true = np.array([0, 1, 0, 1])\n",
    "        y_pred = np.array([0, 0, 0, 1])\n",
    "        metrics = compute_classification_metrics(y_true, y_pred)\n",
    "        self.assertEqual(metrics['accuracy'], 0.75)\n",
    "        self.assertEqual(metrics['precision'], 1.0)\n",
    "        self.assertEqual(metrics['recall'], 0.5)\n",
    "\n",
    "    def test_report_classification_metrics_output(self):\n",
    "        \"\"\"Test that the metrics report is printed correctly.\"\"\"\n",
    "        metrics = {\n",
    "            \"accuracy\": 0.85,\n",
    "            \"precision\": 0.75,\n",
    "            \"recall\": 0.7,\n",
    "            \"f1_score\": 0.72,\n",
    "            \"confusion_matrix\": [[80, 5], [10, 5]],\n",
    "            \"classification_report\": \"Some classification report text\"\n",
    "        }\n",
    "        with patch('builtins.print') as mock_print:\n",
    "            report_classification_metrics(metrics)\n",
    "            printed = \"\\n\".join([call.args[0] for call in mock_print.call_args_list])\n",
    "            self.assertIn(\"CLASSIFICATION METRICS\", printed)\n",
    "            self.assertIn(\"Accuracy : 0.8500\", printed)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_unit_test_description",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "This script provides a crucial **unit test** for the `pipeline.py` file, specifically the `run_churn_pipeline` function. The primary objective is to verify the correct **orchestration and data flow** of the entire machine learning pipeline in **complete isolation**. This is achieved by extensively using mock objects, which act as \"stunt doubles\" for all external and internal function calls that `run_churn_pipeline` depends on. This allows the test to focus solely on the internal logic of the pipeline function the sequence of calls and how data is passed between them without executing any heavy data loading, cleaning, transformation, or model training code.\n",
    "\n",
    "-   **The Role of Mock Objects in This Test:**\n",
    "    -   **`@patch(...)` Decorators:** Each `@patch` decorator intercepts a function call within `run_churn_pipeline`. For instance, `@patch('src.pipeline.load_churn_dataset')` ensures that when `run_churn_pipeline` attempts to call the real `load_churn_dataset` function, it instead calls a special mock object (`mock_load_data`). This is done for every dependency of the pipeline function.\n",
    "    -   **`MagicMock(spec=...)`:** These lines create placeholder objects that mimic real data structures (e.g., `mock_raw_df = MagicMock(spec=pd.DataFrame)`). The `spec` argument is a safety feature that ensures the mock behaves like a real object of that type, preventing accidental calls to non existent attributes or methods.\n",
    "    -   **`.return_value`:** This is crucial for simulating the flow of data. It tells a mock what to \"return\" when it's called. For example, `mock_load_data.return_value = mock_raw_df` means, \"When the mocked `load_churn_dataset` function is called, it should return our placeholder `mock_raw_df` object.\" This creates a chain where the output of one mocked step becomes the input for the next, simulating data handoff.\n",
    "    -   **Configuring Mock Behavior:** Sometimes, the code under test calls methods on the objects it receives. Lines like `mock_y_train.mean.return_value = 0.25` configure these mock objects to return specific values when their methods are called (e.g., `y_train.mean()`), preventing `TypeError` and allowing the pipeline's internal logic (like printing metrics) to execute correctly.\n",
    "\n",
    "-   **`TestPipeline` Class:**\n",
    "    -   **Objective:** Contains unit tests for the `run_churn_pipeline` function.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Prepares a temporary directory (`self.tmp_dir`) to simulate where data files might exist and where models/logs would be stored (`self.model_store_path`). This ensures the test can provide valid (mocked) paths to `run_churn_pipeline` without affecting the actual file system.\n",
    "    -   **`tearDown(self)` method:**\n",
    "        -   **Purpose:** Cleans up the temporary directory created in `setUp` after each test, ensuring a clean state for subsequent tests.\n",
    "\n",
    "    -   **`test_pipeline_unit_orchestration_success`:**\n",
    "        -   **Objective:** This is the primary unit test. It mocks *all* dependencies of `run_churn_pipeline` to focus purely on the function's orchestration logic and data handoff between conceptual stages.\n",
    "        -   **Arrange (Mock Setup):** Numerous `MagicMock` objects are created to represent the outputs of each stage (e.g., raw DataFrame, cleaned DataFrame, transformed DataFrame, X/y splits, model instance). The `.return_value` of each mocked function (e.g., `mock_load_data`, `mock_clean_data`) is set to return these specific mock objects, forming a controlled data flow. Additionally, mock methods like `mock_y_train.mean` are configured to return specific values to avoid runtime errors.\n",
    "        -   **Act:** The `run_churn_pipeline` function is called with mocked file paths and configuration variables imported directly from `src.config` (ensuring consistency with the application's actual configuration).\n",
    "        -   **Assert (`assert_called_once_with`):** This is the most important part. `assert_called_once_with()` is a powerful assertion that verifies two critical things:\n",
    "            1.  **Call Count:** Was the mocked function called exactly one time? (e.g., `mock_load_data.assert_called_once_with(...)`).\n",
    "            2.  **Arguments:** Was it called with the specific arguments you expected? This is crucial for confirming that the output of one mocked step was correctly passed as input to the next (e.g., `mock_clean_data.assert_called_once_with(mock_raw_df, ...)`).\n",
    "            -   Assertions also verify that the `ChurnPredictionModel` was instantiated with the correct preprocessor, its `fit` method was called with the training data, `predict` was called, metrics were computed and reported, and finally, the model and logs were instructed to be saved.\n",
    "            -   The test also asserts that `run_churn_pipeline` returns the mocked model instance and metrics as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/test_pipeline.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains an integration-style test for the `src/pipeline.py` module,\n",
    "    now implemented using Python's built-in `unittest` framework.\n",
    "    It focuses on verifying the correct orchestration and data flow of the entire ML pipeline.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_pipeline_unit_orchestration_success:\n",
    "        - Mocks *all* external and internal function calls to isolate the\n",
    "          `run_churn_pipeline` function.\n",
    "        - Verifies that each step of the pipeline is called in the correct order.\n",
    "        - Asserts that the output of each step is correctly passed as input\n",
    "          to the subsequent step, confirming the data handoff logic.\n",
    "        - Verifies that the `run_churn_pipeline` function returns the trained model\n",
    "          and evaluation metrics as expected.\n",
    "        - Confirms that the model and log saving methods are invoked.\n",
    "    - test_run_churn_pipeline_error_handling:\n",
    "        - Simulates an error at an early stage in the pipeline (e.g., data loading).\n",
    "        - Asserts that the `run_churn_pipeline` correctly propagates or re-raises the error.\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from unittest.mock import patch, MagicMock\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "# Import functions to be tested (and internal functions that will now run unmocked)\n",
    "from src.pipeline import run_churn_pipeline\n",
    "from src.model import ChurnPredictionModel # Import the actual class for spec\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from src.config import (\n",
    "    TARGET_COLUMN, NUMERIC_COLUMNS, CATEGORICAL_COLUMNS,\n",
    "    TEST_SIZE, RANDOM_STATE, MODEL_FILENAME, LOG_FILENAME\n",
    ")\n",
    "\n",
    "\n",
    "class TestPipeline(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create a temporary directory for test outputs\n",
    "        self.tmp_dir = tempfile.mkdtemp()\n",
    "        self.data_path = os.path.join(self.tmp_dir, \"test.csv\")\n",
    "        self.model_store_path = os.path.join(self.tmp_dir, \"model_store\")\n",
    "\n",
    "        # The pipeline expects the model store directory to exist.\n",
    "        os.makedirs(self.model_store_path, exist_ok=True)\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Clean up the temporary directory\n",
    "        shutil.rmtree(self.tmp_dir)\n",
    "\n",
    "    # --- Unit Test: Mock all dependencies to test orchestration ---\n",
    "    @patch('src.pipeline.report_classification_metrics')\n",
    "    @patch('src.pipeline.compute_classification_metrics')\n",
    "    @patch('src.pipeline.ChurnPredictionModel')\n",
    "    @patch('src.pipeline.stratified_split')\n",
    "    @patch('src.pipeline.split_features_and_target')\n",
    "    @patch('src.pipeline.transform_features')\n",
    "    @patch('src.pipeline.clean_churn_data')\n",
    "    @patch('src.pipeline.load_churn_dataset')\n",
    "    def test_pipeline_unit_orchestration_success(\n",
    "        self,\n",
    "        mock_load_data,\n",
    "        mock_clean_data,\n",
    "        mock_transform_features,\n",
    "        mock_split_features,\n",
    "        mock_stratified_split,\n",
    "        mock_churn_model_class,\n",
    "        mock_compute_metrics,\n",
    "        mock_report_metrics,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Test the successful orchestration flow of the pipeline as a pure unit test.\n",
    "        \"\"\"\n",
    "        # Arrange: Create mock objects for each stage's output\n",
    "        mock_raw_df = MagicMock(spec=pd.DataFrame)\n",
    "        mock_clean_df = MagicMock(spec=pd.DataFrame)\n",
    "        mock_transformed_df = MagicMock(spec=pd.DataFrame)\n",
    "        mock_preprocessor = MagicMock(spec=ColumnTransformer)\n",
    "        mock_X, mock_y = MagicMock(spec=pd.DataFrame), MagicMock(spec=pd.Series)\n",
    "        mock_X_train, mock_X_test = MagicMock(spec=pd.DataFrame), MagicMock(spec=pd.DataFrame)\n",
    "        mock_y_train, mock_y_test = MagicMock(spec=pd.Series), MagicMock(spec=pd.Series)\n",
    "\n",
    "        # Configure mocks to handle method calls inside the pipeline that would otherwise fail\n",
    "        mock_y_train.mean.return_value = 0.25  # Return a float for the .mean() call\n",
    "        mock_y_test.mean.return_value = 0.25\n",
    "        mock_y_test.to_numpy.return_value = np.array([0, 1, 1, 0]) # Return a numpy array for the .to_numpy() call\n",
    "\n",
    "        # Arrange: Define the return values for the mocked functions\n",
    "        mock_load_data.return_value = mock_raw_df\n",
    "        mock_clean_data.return_value = mock_clean_df\n",
    "        mock_transform_features.return_value = (mock_transformed_df, mock_preprocessor)\n",
    "        mock_split_features.return_value = (mock_X, mock_y)\n",
    "        mock_stratified_split.return_value = (mock_X_train, mock_X_test, mock_y_train, mock_y_test)\n",
    "\n",
    "        # Arrange: Set up the mock model *instance* returned by the mocked class\n",
    "        mock_model_instance = MagicMock(spec=ChurnPredictionModel)\n",
    "        mock_model_instance.predict.return_value = np.array([0, 1])\n",
    "        mock_churn_model_class.return_value = mock_model_instance # What the constructor returns\n",
    "\n",
    "        # Arrange: Set up mock for compute_classification_metrics\n",
    "        mock_compute_metrics.return_value = {\"accuracy\": 0.95, \"f1_score\": 0.90}\n",
    "\n",
    "        # Act\n",
    "        returned_model, returned_metrics = run_churn_pipeline(\n",
    "            data_file_path=self.data_path,\n",
    "            target_column=TARGET_COLUMN,\n",
    "            numeric_columns=NUMERIC_COLUMNS,\n",
    "            categorical_columns=CATEGORICAL_COLUMNS,\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=RANDOM_STATE,\n",
    "            model_dir_path=self.model_store_path,\n",
    "            model_filename=MODEL_FILENAME,\n",
    "            log_filename=LOG_FILENAME\n",
    "        )\n",
    "\n",
    "        # Assert: Verify the orchestration and data handoffs\n",
    "        mock_load_data.assert_called_once_with(self.data_path)\n",
    "        mock_clean_data.assert_called_once_with(mock_raw_df, TARGET_COLUMN, NUMERIC_COLUMNS, CATEGORICAL_COLUMNS)\n",
    "        mock_transform_features.assert_called_once_with(mock_clean_df, TARGET_COLUMN, NUMERIC_COLUMNS, CATEGORICAL_COLUMNS)\n",
    "        mock_split_features.assert_called_once_with(mock_transformed_df)\n",
    "        mock_stratified_split.assert_called_once_with(mock_X, mock_y, test_size=TEST_SIZE, seed=RANDOM_STATE)\n",
    "\n",
    "        # Assert model initialization and training\n",
    "        mock_churn_model_class.assert_called_once_with(preprocessor=mock_preprocessor, random_state=RANDOM_STATE)\n",
    "        mock_model_instance.fit.assert_called_once_with(mock_X_train, mock_y_train)\n",
    "\n",
    "        # Assert prediction and evaluation\n",
    "        mock_model_instance.predict.assert_called_once_with(mock_X_test)\n",
    "        mock_compute_metrics.assert_called_once()\n",
    "        # Check that the correct y_true and y_pred were passed to metrics\n",
    "        y_true_arg = mock_compute_metrics.call_args[0][0]\n",
    "        y_pred_arg = mock_compute_metrics.call_args[0][1]\n",
    "        self.assertIs(y_true_arg, mock_y_test.to_numpy.return_value)\n",
    "        self.assertIs(y_pred_arg, mock_model_instance.predict.return_value)\n",
    "\n",
    "        mock_report_metrics.assert_called_once_with(mock_compute_metrics.return_value)\n",
    "\n",
    "        # Assert artifacts were saved\n",
    "        mock_model_instance.save.assert_called_once_with(os.path.join(self.model_store_path, MODEL_FILENAME))\n",
    "        mock_model_instance.log_run.assert_called_once()\n",
    "\n",
    "        self.assertIs(returned_model, mock_model_instance)\n",
    "        self.assertEqual(returned_metrics, {\"accuracy\": 0.95, \"f1_score\": 0.90})\n",
    "\n",
    "    def test_run_churn_pipeline_error_handling(self):\n",
    "        \"\"\"Test that run_churn_pipeline correctly handles exceptions.\"\"\"\n",
    "        with patch('src.pipeline.load_churn_dataset', side_effect=RuntimeError(\"Test load error\")):\n",
    "            with self.assertRaisesRegex(RuntimeError, \"Test load error\"):\n",
    "                run_churn_pipeline(\n",
    "                    data_file_path=\"dummy_path.csv\",\n",
    "                    target_column=TARGET_COLUMN,\n",
    "                    numeric_columns=NUMERIC_COLUMNS,\n",
    "                    categorical_columns=CATEGORICAL_COLUMNS,\n",
    "                    test_size=TEST_SIZE,\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    model_dir_path=self.model_store_path, # Use self.model_store_path from setUp\n",
    "                    model_filename=MODEL_FILENAME,\n",
    "                    log_filename=LOG_FILENAME\n",
    "                )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_import_and_test_philosophy",
   "metadata": {},
   "source": [
    "In this scenario, it is not only correct but also highly recommended to import and use the variables from your `config.py` file in your pipeline test.\n",
    "\n",
    "The `run_churn_pipeline` function is designed to be configured by the values in `config.py`. Your test should validate that the pipeline behaves correctly with that specific configuration. If you were to hardcode values like `target_column='Churn'` in your test, and later someone changed the `TARGET_COLUMN` in the config file, your test would be validating an outdated scenario. By importing from `config.py`, your test and your application logic remain perfectly in sync.\n",
    "\n",
    "**DRY (Don't Repeat Yourself)** prevents you from defining the same constants in both your application code and your test code. This reduces the chance of typos and makes the project easier to maintain. If a configuration value needs to change, you only have to change it in one place: `config.py`.\n",
    "\n",
    "Using the config variables makes the test's purpose clear: \"I am verifying that the pipeline correctly orchestrates its steps using the project's standard configuration.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_unit_test_description",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "This script contains **unit tests** for the `main.py` entry point of the application, utilizing Python's built in `unittest` framework. The primary goal of these tests is to ensure that the `main` function correctly orchestrates the machine learning pipeline execution based on the provided parameters, and that it handles errors gracefully. These tests are crucial for verifying the top level control flow of your application in isolation.\n",
    "\n",
    "-   **`TestMainFunction` Class:**\n",
    "    -   **Objective:** Provides isolated unit tests specifically for the `main` function, confirming its role as the pipeline orchestrator.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Initializes a temporary directory (`self.output_dir`) before each test. This directory acts as a safe, isolated space for `main.py` to create its output artifacts (like models and logs) without affecting the actual file system or other tests.\n",
    "    -   **`tearDown(self)` method:**\n",
    "        -   **Purpose:** Cleans up the temporary directory created in `setUp` after each test has completed. This ensures that tests are independent and do not leave behind any residual files, promoting a clean testing environment.\n",
    "\n",
    "-   **`test_main_success`:**\n",
    "    -   **Objective:** Verifies that the `main` function correctly prepares input paths and parameters, then successfully calls the `run_churn_pipeline` function with these details.\n",
    "    -   **Key Techniques:**\n",
    "        -   `@patch('main.run_churn_pipeline')`: This decorator replaces the actual `run_churn_pipeline` function with a `MagicMock` object during the test. This is critical for unit testing `main` in isolation, preventing the test from executing the entire, potentially time consuming, ML pipeline.\n",
    "        -   `mock_run_pipeline.return_value = (dummy_model, dummy_metrics)`: Configures the mock to return dummy values, simulating a successful pipeline run, allowing the test to verify `main`'s handling of the pipeline's return values.\n",
    "        -   `with patch('builtins.print')`: Suppresses `print` statements within `main` during the test to keep the test output clean and focused on assertions.\n",
    "    -   **Assertions:**\n",
    "        -   It constructs the `expected_data_path` and `expected_model_store_path` dynamically based on the temporary output directory and predefined configuration constants (e.g., `DATA_DIR_NAME`, `DATASET_FILENAME`).\n",
    "        -   `mock_run_pipeline.assert_called_once_with(...)`: This is the core assertion. It verifies that `run_churn_pipeline` was called exactly once and, crucially, that all its arguments (data paths, column names, test size, random state, model/log paths) match the expected values derived from the configuration. This ensures `main` passes the correct information down to the pipeline.\n",
    "\n",
    "-   **`test_main_pipeline_error_handling`:**\n",
    "    -   **Objective:** Tests that the `main` function correctly handles and propagates exceptions that might occur within the `run_churn_pipeline`.\n",
    "    -   **Key Technique:**\n",
    "        -   `@patch('main.run_churn_pipeline', side_effect=ValueError(\"Simulated pipeline error\"))`: This decorator configures the mocked `run_churn_pipeline` to raise a specific `ValueError` whenever it's called. This simulates a failure within the pipeline (e.g., a data loading error or training crash).\n",
    "    -   **Assertions:**\n",
    "        -   `with self.assertRaisesRegex(ValueError, \"Simulated pipeline error\")`: This assertion confirms that calling `main` results in the expected `ValueError` being reraised. This demonstrates that `main` does not silently swallow errors from its dependencies, which is essential for proper debugging and error reporting in a real application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba49af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains unit tests for the main entrypoint script `main.py`,\n",
    "    implemented using Python's built-in `unittest` framework.\n",
    "    It verifies that the main function correctly orchestrates the pipeline\n",
    "    execution based on its input parameters.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_main_success:\n",
    "        - Mocks the `run_churn_pipeline` function to isolate the `main` function's logic.\n",
    "        - Calls `main` with a specified output directory.\n",
    "        - Asserts that `run_churn_pipeline` is called exactly once with the correctly\n",
    "          constructed file paths and configuration values.\n",
    "    - test_main_pipeline_error_handling:\n",
    "        - Simulates an error occurring within `run_churn_pipeline`.\n",
    "        - Asserts that the `main` function correctly catches and re-raises the\n",
    "          exception, ensuring proper error propagation.\n",
    "\"\"\"\n",
    "import unittest\n",
    "from unittest.mock import patch, MagicMock\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "\n",
    "# Add project root to sys.path for import resolution\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "from main import main\n",
    "from src.config import (\n",
    "    TARGET_COLUMN, NUMERIC_COLUMNS, CATEGORICAL_COLUMNS,\n",
    "    TEST_SIZE, RANDOM_STATE, MODEL_FILENAME, LOG_FILENAME,\n",
    "    DATA_DIR_NAME, RAW_DATA_DIR_NAME, DATASET_FILENAME, MODEL_STORE_DIR\n",
    ")\n",
    "\n",
    "class TestMainFunction(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up a temporary directory for test artifacts.\"\"\"\n",
    "        # We manually manage the TemporaryDirectory to use it across the test method.\n",
    "        self.tmpdir_context = tempfile.TemporaryDirectory()\n",
    "        self.output_dir = Path(self.tmpdir_context.name)\n",
    "\n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up the temporary directory after the test.\"\"\"\n",
    "        self.tmpdir_context.cleanup()\n",
    "\n",
    "    @patch('main.run_churn_pipeline')\n",
    "    def test_main_success(self, mock_run_pipeline):\n",
    "        \"\"\"\n",
    "        Test that `main` correctly constructs paths and calls the pipeline\n",
    "        when an output directory is provided.\n",
    "        \"\"\"\n",
    "        dummy_model = MagicMock()\n",
    "        dummy_metrics = {\"accuracy\": 0.92}\n",
    "        mock_run_pipeline.return_value = (dummy_model, dummy_metrics)\n",
    "\n",
    "        with patch('builtins.print'): # Suppress print statements for a cleaner test output\n",
    "            main(output_base_dir=self.output_dir)\n",
    "\n",
    "        expected_data_path = str(self.output_dir / DATA_DIR_NAME / RAW_DATA_DIR_NAME / DATASET_FILENAME)\n",
    "        expected_model_store_path = str(self.output_dir / MODEL_STORE_DIR)\n",
    "\n",
    "        mock_run_pipeline.assert_called_once_with(\n",
    "            data_file_path=expected_data_path,\n",
    "            target_column=TARGET_COLUMN,\n",
    "            numeric_columns=NUMERIC_COLUMNS,\n",
    "            categorical_columns=CATEGORICAL_COLUMNS,\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=RANDOM_STATE,\n",
    "            model_dir_path=expected_model_store_path,\n",
    "            model_filename=MODEL_FILENAME,\n",
    "            log_filename=LOG_FILENAME\n",
    "        )\n",
    "\n",
    "    @patch('main.run_churn_pipeline', side_effect=ValueError(\"Simulated pipeline error\"))\n",
    "    def test_main_pipeline_error_handling(self, mock_run_pipeline):\n",
    "        \"\"\"Test that `main` propagates exceptions raised from the pipeline.\"\"\"\n",
    "        # We expect the ValueError from the mock to be re-raised by main()\n",
    "        with self.assertRaisesRegex(ValueError, \"Simulated pipeline error\"):\n",
    "            main(output_base_dir=self.output_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integration_test_description",
   "metadata": {},
   "source": [
    "# Integration test:\n",
    "\n",
    "### The Purpose of a Unit Test vs. an Integration Test:\n",
    "\n",
    "The goal of a unit test is to test a single piece of code in this case, the orchestration logic inside the `run_churn_pipeline` function in complete isolation. We mock all its dependencies (the other functions it calls) to make assumptions about their behavior. The test asks: \"Assuming `stratified_split` gives me a `y_train` object, and assuming that object's `.mean()` method returns a number, does my `run_churn_pipeline` function correctly use that number in the `print()` statement without crashing?\"\n",
    "\n",
    "The goal of an integration test is to test how multiple components work together. In an integration test, we would not mock `stratified_split`. We would let it run and produce a real pandas Series for `y_train`. Then, when `.mean()` is called, it would be the real pandas calculation. This tests the \"integration\" between the splitting function and the pipeline's logging.\n",
    "\n",
    "## Data Preprocessing Integration\n",
    "\n",
    "Its purpose is to verify that two separate components of your project, the `load_churn_dataset` function and the `clean_churn_data` function work together correctly as a single unit. It ensures that the data produced by the loader is in the exact format that the cleaner expects.\n",
    "\n",
    "-   **Test Setup (`setUp` method):**\n",
    "    -   `setUp(self)`: Before the test is run, this method prepares the environment.\n",
    "    -   **Create a Temporary Directory:** It creates a new, empty temporary directory. This is a best practice that gives the test a clean, isolated space to work in, preventing it from interfering with other files on your system.\n",
    "    -   **Create Realistic Test Data:** It defines a multi-line string that looks exactly like a real CSV file. This data is crucial because it's a representative sample of what the pipeline will see, including important edge cases like an empty value for TotalCharges (notice the `,,` in the fourth row).\n",
    "    -   **Write the Test File:** It writes this string data into a new CSV file inside the temporary directory.\n",
    "\n",
    "-   **The Integration Test (`test_data_loading_and_cleaning_flow` method):** This is the main test that verifies the integration between the two functions.\n",
    "    -   **Load the Data:** The test first calls the real `load_churn_dataset` function, passing it the path to the temporary CSV file created during setup. It then asserts that this function successfully returned a pandas DataFrame.\n",
    "    -   **Clean the Data:** takes the `raw_df` produced by the loading step and passes it directly to the `clean_churn_data` function. This is the \"integration\" step testing the handoff between the two components.\n",
    "-   **Assert Results:** The test performs a series of checks on the `cleaned_df` to confirm that the entire process worked as expected:\n",
    "    -   It verifies that the `churn_binary` column was created correctly.\n",
    "    -   It checks that the `TotalCharges` column, which contained an empty string, was correctly converted to a numeric type and that the empty value is now a proper `NaN` missing value.\n",
    "    -   It ensures that the final set of columns in the cleaned DataFrame is exactly what it should be, based on the project's configuration files.\n",
    "\n",
    "-   **Test Cleanup (`tearDown` method):** After the test finishes, this method is automatically called. It deletes the entire temporary directory and the CSV file inside it, ensuring that the test leaves no trace behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ab9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/test_preprocessing.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains unit tests for the functions in `src/preprocessing.py`,\n",
    "    now implemented using Python's built-in `unittest` framework.\n",
    "    It ensures that data cleaning, preprocessing pipeline construction,\n",
    "    feature transformation, and data splitting are performed correctly.\n",
    "\n",
    "Tests Performed:\n",
    "    - clean_churn_data:\n",
    "        - Verifies successful cleaning, type conversion (e.g., TotalCharges),\n",
    "          creation of churn_binary, and correct column selection.\n",
    "        - Checks for proper error handling when essential columns are missing.\n",
    "    - build_preprocessing_pipeline:\n",
    "        - Asserts that the ColumnTransformer is correctly built with expected\n",
    "          numerical and categorical transformers (StandardScaler, OneHotEncoder).\n",
    "    - transform_features:\n",
    "        - Confirms that target encoding is applied and that DataFrame attributes\n",
    "          (like feature_columns, preprocessor) are correctly set.\n",
    "    - split_features_and_target:\n",
    "        - Ensures that features (X) and target (y) are accurately separated\n",
    "          based on DataFrame attributes.\n",
    "        - Checks for error when required attributes are missing.\n",
    "    - stratified_split:\n",
    "        - Verifies that the data is split into train and test sets while\n",
    "          maintaining the original class proportions (stratification).\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "# Import functions to be tested\n",
    "from src.preprocessing import (\n",
    "    clean_churn_data,\n",
    "    build_preprocessing_pipeline,\n",
    "    transform_features,\n",
    "    split_features_and_target,\n",
    "    stratified_split\n",
    ")\n",
    "\n",
    "class TestPreprocessing(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Helper method for raw DataFrame\n",
    "        self.raw_df = pd.DataFrame({\n",
    "            'customerID': ['1', '2', '3', '4'],\n",
    "            'gender': ['Male', 'Female', 'Male', 'Female'],\n",
    "            'SeniorCitizen': [0, 1, 0, 0],\n",
    "            'Partner': ['Yes', 'No', 'No', 'Yes'],\n",
    "            'Dependents': ['No', 'Yes', 'No', 'No'],\n",
    "            'tenure': [1, 34, 2, 45],\n",
    "            'PhoneService': ['No', 'Yes', 'Yes', 'Yes'],\n",
    "            'MultipleLines': ['No phone service', 'No', 'No', 'Yes'],\n",
    "            'InternetService': ['DSL', 'DSL', 'Fiber optic', 'DSL'],\n",
    "            'OnlineSecurity': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'OnlineBackup': ['Yes', 'No', 'Yes', 'No'],\n",
    "            'DeviceProtection': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'TechSupport': ['No', 'No', 'No', 'Yes'],\n",
    "            'StreamingTV': ['No', 'No', 'No', 'Yes'],\n",
    "            'StreamingMovies': ['No', 'No', 'No', 'Yes'],\n",
    "            'Contract': ['Month-to-month', 'One year', 'Month-to-month', 'Two year'],\n",
    "            'PaperlessBilling': ['Yes', 'No', 'Yes', 'No'],\n",
    "            'PaymentMethod': ['Electronic check', 'Mailed check', 'Electronic check', 'Bank transfer (automatic)'],\n",
    "            'MonthlyCharges': [29.85, 56.95, 53.85, 70.70],\n",
    "            'TotalCharges': ['29.85', '1889.5', '108.15', ''], # Empty string for TotalCharges\n",
    "            'Churn': ['No', 'No', 'Yes', 'Yes']\n",
    "        })\n",
    "\n",
    "        # Helper method for configuration columns\n",
    "        self.config_columns = {\n",
    "            'TARGET_COLUMN': 'Churn',\n",
    "            'NUMERIC_COLUMNS': ['tenure', 'MonthlyCharges', 'TotalCharges'],\n",
    "            'CATEGORICAL_COLUMNS': [\n",
    "                'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "                'PhoneService', 'MultipleLines', 'InternetService',\n",
    "                'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "                'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Create cleaned and transformed dataframes once for all tests\n",
    "        self.cleaned_df = clean_churn_data(\n",
    "            self.raw_df.copy(),\n",
    "            self.config_columns['TARGET_COLUMN'],\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "        \n",
    "        self.transformed_df, _ = transform_features(\n",
    "            self.cleaned_df.copy(),\n",
    "            self.config_columns['TARGET_COLUMN'],\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "\n",
    "    # --- Tests for clean_churn_data ---\n",
    "    def test_clean_churn_data_success(self):\n",
    "        \"\"\"\n",
    "        Test successful data cleaning.\n",
    "        Verifies that:\n",
    "        1. The target column 'Churn' is converted to a binary 'churn_binary'.\n",
    "        2. 'TotalCharges' is converted to a numeric type, handling non-numeric values as NaN.\n",
    "        3. The final DataFrame contains only the expected columns.\n",
    "        \"\"\"\n",
    "        df_clean = self.cleaned_df\n",
    "\n",
    "        # 1. Verify target column transformation\n",
    "        self.assertIn('churn_binary', df_clean.columns)\n",
    "        self.assertEqual(df_clean['churn_binary'].dtype, np.int64)\n",
    "        pd.testing.assert_series_equal(\n",
    "            df_clean['churn_binary'],\n",
    "            pd.Series([0, 0, 1, 1], name='churn_binary'),\n",
    "            check_index=False  # We only care about the values\n",
    "        )\n",
    "\n",
    "        # 2. Verify 'TotalCharges' transformation\n",
    "        self.assertTrue(pd.api.types.is_numeric_dtype(df_clean['TotalCharges']))\n",
    "        self.assertTrue(np.isnan(df_clean['TotalCharges'].iloc[3]))\n",
    "\n",
    "        # 3. Verify final column set\n",
    "        expected_cols = self.config_columns['NUMERIC_COLUMNS'] + self.config_columns['CATEGORICAL_COLUMNS'] + ['churn_binary']\n",
    "        self.assertEqual(set(df_clean.columns), set(expected_cols))\n",
    "\n",
    "    def test_clean_churn_data_missing_target_column(self):\n",
    "        \"\"\"Test error handling when target column is missing.\"\"\"\n",
    "        df_missing_target = self.raw_df.drop(columns=[self.config_columns['TARGET_COLUMN']]).copy()\n",
    "        with self.assertRaisesRegex(ValueError, \"Missing required columns in dataset: \\\\['Churn'\\\\]\"):\n",
    "            clean_churn_data(\n",
    "                df_missing_target,\n",
    "                self.config_columns['TARGET_COLUMN'],\n",
    "                self.config_columns['NUMERIC_COLUMNS'],\n",
    "                self.config_columns['CATEGORICAL_COLUMNS']\n",
    "            )\n",
    "\n",
    "    # --- Tests for build_preprocessing_pipeline ---\n",
    "    def test_build_preprocessing_pipeline_structure(self):\n",
    "        \"\"\"\n",
    "        Test the structure of the preprocessing pipeline.\n",
    "        Ensures the ColumnTransformer is built with the correct transformers\n",
    "        for numeric and categorical features. This is key for ensuring our model's inputs are consistent.\n",
    "        \"\"\"\n",
    "        preprocessor = build_preprocessing_pipeline(\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "\n",
    "        self.assertIsInstance(preprocessor, ColumnTransformer)\n",
    "        self.assertEqual(len(preprocessor.transformers), 2) # type: ignore\n",
    "\n",
    "\n",
    "        num_transformer = [t for name, t, cols in preprocessor.transformers if name == 'num'][0]  # type: ignore\n",
    "        cat_transformer = [t for name, t, cols in preprocessor.transformers if name == 'cat'][0]  # type: ignore\n",
    "\n",
    "        self.assertIsInstance(num_transformer, Pipeline)\n",
    "        self.assertIsInstance(cat_transformer, Pipeline)\n",
    "\n",
    "        self.assertIn('scaler', num_transformer.named_steps)\n",
    "        self.assertIsInstance(num_transformer.named_steps['scaler'], StandardScaler)\n",
    "        self.assertIn('onehot', cat_transformer.named_steps)\n",
    "        self.assertIsInstance(cat_transformer.named_steps['onehot'], OneHotEncoder)\n",
    "\n",
    "    # --- Tests for transform_features ---\n",
    "    def test_transform_features_success(self):\n",
    "        \"\"\"\n",
    "        Test successful feature transformation and metadata attachment.\n",
    "        Verifies that the function correctly encodes the target and, crucially for MLOps,\n",
    "        attaches important metadata (like feature names and the preprocessor itself)\n",
    "        to the DataFrame's attributes for later use in the pipeline.\n",
    "        \"\"\"\n",
    "        df_transformed, preprocessor = transform_features(\n",
    "            self.cleaned_df.copy(),\n",
    "            self.config_columns['TARGET_COLUMN'],\n",
    "            self.config_columns['NUMERIC_COLUMNS'],\n",
    "            self.config_columns['CATEGORICAL_COLUMNS']\n",
    "        )\n",
    "\n",
    "        self.assertIsInstance(df_transformed, pd.DataFrame)\n",
    "        self.assertIn('target_encoded', df_transformed.columns)\n",
    "        self.assertEqual(df_transformed['target_encoded'].dtype, np.int64)\n",
    "        self.assertIsInstance(preprocessor, ColumnTransformer)\n",
    "\n",
    "        self.assertIn('feature_columns', df_transformed.attrs)\n",
    "        self.assertIn('preprocessor', df_transformed.attrs)\n",
    "        self.assertIn('target_mapping', df_transformed.attrs)\n",
    "        self.assertIn('target_names', df_transformed.attrs)\n",
    "        self.assertEqual(set(df_transformed.attrs['feature_columns']), set(self.config_columns['NUMERIC_COLUMNS'] + self.config_columns['CATEGORICAL_COLUMNS']))\n",
    "\n",
    "    # --- Tests for split_features_and_target ---\n",
    "    def test_split_features_and_target_success(self):\n",
    "        \"\"\"Test successful splitting of features and target.\"\"\"\n",
    "        X, y = split_features_and_target(self.transformed_df)\n",
    "\n",
    "        self.assertIsInstance(X, pd.DataFrame)\n",
    "        self.assertIsInstance(y, pd.Series)\n",
    "        self.assertEqual(len(X), len(y))\n",
    "        self.assertNotIn('target_encoded', X.columns)\n",
    "        self.assertEqual(y.name, 'target_encoded')\n",
    "        self.assertEqual(set(X.columns), set(self.transformed_df.attrs['feature_columns']))\n",
    "\n",
    "    def test_split_features_and_target_missing_attrs(self):\n",
    "        \"\"\"Test error when feature_columns attribute is missing.\"\"\"\n",
    "        df_no_attrs = self.cleaned_df.copy()\n",
    "        if 'feature_columns' in df_no_attrs.attrs:\n",
    "            del df_no_attrs.attrs['feature_columns']\n",
    "\n",
    "        with self.assertRaisesRegex(ValueError, \"No feature columns found in dataset attributes\"):\n",
    "            split_features_and_target(df_no_attrs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall_preprocessing_transform_integration_summary",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline and Transform Integration\n",
    "\n",
    "This script is a crucial **integration test** for your preprocessing logic, specifically for the `build_preprocessing_pipeline` and `transform_features` functions. Its main purpose is to verify that these two components work together correctly, ensuring that the `ColumnTransformer` pipeline is constructed properly and then successfully applied to clean data, producing a fully transformed DataFrame with all the necessary metadata for the next steps in your MLOps workflow.\n",
    "\n",
    "-   **Test Setup (`setUp` method):**\n",
    "    -   `setUp(self)`: Before the test is run, this method creates a single, important piece of test data: `self.dummy_cleaned_df`.\n",
    "    -   **Simulates Clean Data:** This DataFrame is designed to look exactly like the output of the `clean_churn_data` function. It's a \"clean\" dataset ready for transformation.\n",
    "    -   **Includes Edge Cases:** Crucially, it includes a `np.nan` (Not a Number) value in the `TotalCharges` column. This is essential for verifying that the imputation step (filling in missing values) of the pipeline works correctly.\n",
    "\n",
    "-   **The Integration Test (`test_preprocessing_pipeline_and_transform_flow` method):**\n",
    "    -   **Build the Preprocessing Pipeline:** The test first calls the `build_preprocessing_pipeline` function. This creates the `scikit-learn` `ColumnTransformer` object, which knows how to handle numeric and categorical columns differently.\n",
    "    -   **Transform the Features:** The test calls `transform_features`, passing it a copy of the clean dummy data from `setUp`. Inside this function, the preprocessor from the previous step is fitted to the data and used to transform it.\n",
    "-   **Assert the Results:** The rest of the test consists of a series of assertions to confirm that the entire process worked as expected.\n",
    "    -   **Check DataFrame Structure:** It verifies that the output `df_transformed` is a `DataFrame` and that the target column has been correctly created and encoded as `target_encoded`.\n",
    "    -   **Check Metadata (`.attrs`):** It asserts that critical metadata has been attached to the DataFrame's attributes. This includes the list of feature columns and, most importantly, the `fitted_preprocessor` itself. This is a key MLOps practice, as it ensures that artifacts are passed along with the data they correspond to.\n",
    "    -   **Verify Imputation:** It inspects the `fitted_preprocessor` and confirms that the `SimpleImputer` for numeric columns has been successfully fitted. A fitted imputer will have a `statistics_` attribute where it stores the values it learned (e.g., the median of each column) to fill in missing data. This proves that the `np.nan` value was handled correctly during the `fit_transform` process.\n",
    "    -   **Verify Feature Columns Attribute:** It checks that the `feature_columns` attribute correctly stores the names of the original numeric and categorical features, ensuring consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4378ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/integration/test_preprocessing_transform_integration.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    Integration test for the preprocessing pipeline building and feature transformation,\n",
    "    now implemented using Python's built-in `unittest` framework.\n",
    "    Verifies that the preprocessor built by `build_preprocessing_pipeline`\n",
    "    is correctly applied by `transform_features`, and attributes are set.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_preprocessing_pipeline_and_transform_flow:\n",
    "        Creates a dummy cleaned DataFrame, constructs a preprocessor,\n",
    "        applies transformation, and asserts on the transformed DataFrame's\n",
    "        structure, content, and attached attributes.\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "# Import functions and config\n",
    "from src.preprocessing import build_preprocessing_pipeline, transform_features\n",
    "from src.config import TARGET_COLUMN, NUMERIC_COLUMNS, CATEGORICAL_COLUMNS\n",
    "\n",
    "class TestPreprocessingTransformIntegration(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Provides a dummy cleaned DataFrame for testing.\n",
    "        # This simulates the output of `clean_churn_data`.\n",
    "        self.dummy_cleaned_df = pd.DataFrame({\n",
    "            # Numeric columns\n",
    "            'tenure': [1, 34, 5, 20],\n",
    "            'MonthlyCharges': [29.85, 56.95, 30.15, 80.0],\n",
    "            'TotalCharges': [29.85, 1889.5, np.nan, 1600.0],\n",
    "            # Categorical columns - ensuring all from config are present\n",
    "            'gender': ['Male', 'Female', 'Male', 'Female'],\n",
    "            'SeniorCitizen': [0, 1, 0, 0],\n",
    "            'Partner': ['Yes', 'No', 'No', 'Yes'],\n",
    "            'Dependents': ['No', 'No', 'Yes', 'No'],\n",
    "            'PhoneService': ['No', 'Yes', 'Yes', 'Yes'],\n",
    "            'MultipleLines': ['No phone service', 'No', 'Yes', 'No'],\n",
    "            'InternetService': ['DSL', 'Fiber optic', 'DSL', 'No'],\n",
    "            'OnlineSecurity': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'OnlineBackup': ['Yes', 'No', 'Yes', 'No'],\n",
    "            'DeviceProtection': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'TechSupport': ['No', 'No', 'Yes', 'Yes'],\n",
    "            'StreamingTV': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'StreamingMovies': ['No', 'Yes', 'No', 'Yes'],\n",
    "            'Contract': ['Month-to-month', 'One year', 'Two year', 'Month-to-month'],\n",
    "            'PaperlessBilling': ['Yes', 'No', 'Yes', 'No'],\n",
    "            'PaymentMethod': ['Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'],\n",
    "            # Original Churn column for target encoding\n",
    "            'Churn': ['No', 'Yes', 'No', 'Yes']\n",
    "        })\n",
    "        # Add 'churn_binary' which `clean_churn_data` would add\n",
    "        le = LabelEncoder()\n",
    "        self.dummy_cleaned_df['churn_binary'] = le.fit_transform(self.dummy_cleaned_df['Churn'])\n",
    "        self.dummy_cleaned_df.drop(columns=['Churn'], inplace=True)\n",
    "\n",
    "\n",
    "    def test_preprocessing_pipeline_and_transform_flow(self):\n",
    "        \"\"\"\n",
    "        Integration test: Verifies the flow from cleaned data to preprocessor\n",
    "        building and feature transformation.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Integration Test: Preprocessing Pipeline and Transform Flow ---\")\n",
    "\n",
    "        # 1. Build preprocessing pipeline\n",
    "        preprocessor = build_preprocessing_pipeline(NUMERIC_COLUMNS, CATEGORICAL_COLUMNS)\n",
    "        self.assertIsInstance(preprocessor, ColumnTransformer)\n",
    "        print(\"Preprocessing pipeline built successfully.\")\n",
    "\n",
    "        # 2. Transform features using the built pipeline\n",
    "        df_transformed, fitted_preprocessor = transform_features(\n",
    "            self.dummy_cleaned_df.copy(), # Pass a copy to avoid modifying original\n",
    "            TARGET_COLUMN, # 'Churn' in original, 'churn_binary' in cleaned. transform_features expects original target\n",
    "            NUMERIC_COLUMNS,\n",
    "            CATEGORICAL_COLUMNS\n",
    "        )\n",
    "\n",
    "        # Assertions on the transformed DataFrame\n",
    "        self.assertIsInstance(df_transformed, pd.DataFrame)\n",
    "        self.assertIn('target_encoded', df_transformed.columns)\n",
    "        self.assertTrue(pd.api.types.is_integer_dtype(df_transformed['target_encoded']))\n",
    "        # Verify target encoding is just the churn_binary column\n",
    "        self.assertTrue(df_transformed['target_encoded'].equals(self.dummy_cleaned_df['churn_binary']))\n",
    "        print(f\"DataFrame transformed. Shape: {df_transformed.shape}, Target encoded column present.\")\n",
    "\n",
    "        # Assertions on attributes\n",
    "        self.assertIn('feature_columns', df_transformed.attrs)\n",
    "        self.assertIn('preprocessor', df_transformed.attrs)\n",
    "        self.assertIn('target_mapping', df_transformed.attrs)\n",
    "        self.assertIn('target_names', df_transformed.attrs)\n",
    "        self.assertIsInstance(df_transformed.attrs['preprocessor'], ColumnTransformer)\n",
    "        print(\"DataFrame attributes correctly set.\")\n",
    "\n",
    "        # Check that the imputer within the fitted preprocessor has learned from the data.\n",
    "        # This is a more direct way to verify that the NaN was handled during fitting.\n",
    "        num_pipeline = fitted_preprocessor.named_transformers_['num'] # type: ignore\n",
    "        imputer = num_pipeline.named_steps['imputer']\n",
    "        \n",
    "        # A successfully fitted SimpleImputer will have a `statistics_` attribute.\n",
    "        self.assertTrue(hasattr(imputer, 'statistics_'))\n",
    "        self.assertEqual(len(imputer.statistics_), len(NUMERIC_COLUMNS))\n",
    "        print(\"Numeric imputer was successfully fitted on the data, handling NaNs.\")\n",
    "\n",
    "\n",
    "        # Check that feature_columns attribute stores the input feature names\n",
    "        self.assertEqual(\n",
    "            set(df_transformed.attrs['feature_columns']),\n",
    "            set(NUMERIC_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "        )\n",
    "        print(\"Feature columns attribute correctly stores original feature names.\")\n",
    "        print(\"--- Integration Test Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_metrics_integration_description",
   "metadata": {},
   "source": [
    "## Model Metrics Integration\n",
    "\n",
    "This script provides an **integration test** for the critical flow of the machine learning pipeline: from preprocessed data to model training, prediction, and subsequent metric computation. It uses Python's built-in `unittest` framework to ensure that `ChurnPredictionModel` interacts correctly with its inputs (simulated preprocessed data) and that its outputs (predictions) are accurately processed by the `compute_classification_metrics` function. This test bridges the gap between unit-tested components to verify their combined functionality.\n",
    "\n",
    "-   **`TestModelMetricsIntegration` Class:**\n",
    "    -   **Objective:** Contains the integration test for the model's core lifecycle, ensuring the end-to-end process from processed data to evaluated metrics works as expected.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Prepares a dummy dataset that simulates the output of the preprocessing module. This means features (`X`) are scaled, and crucial metadata (`preprocessor`, `feature_columns`) is attached to the DataFrame's attributes, mimicking the state of data just before model training in a real pipeline.\n",
    "        -   **`self.X_full` and `self.y_full`:** Randomly generated features and a binary target are created. `X_full` then undergoes `StandardScaler` transformation, and the preprocessor instance and feature column names are explicitly stored in `X_full.attrs`. This setup ensures the data is in the expected format for the `ChurnPredictionModel`.\n",
    "\n",
    "-   **`test_model_training_prediction_and_metrics_flow`:**\n",
    "    -   **Objective:** This is the primary integration test. It verifies the complete flow:\n",
    "        1.  **Data Splitting:** Simulates the splitting of the preprocessed data into training and testing sets using `stratified_split`.\n",
    "        2.  **Model Instantiation and Training:** Initializes `ChurnPredictionModel` using the preprocessor attached to the `X_full` DataFrame's attributes and trains it on the training data.\n",
    "        3.  **Prediction:** Generates predictions on the test set.\n",
    "        4.  **Metric Computation:** Uses `compute_classification_metrics` to evaluate the model's performance based on the predictions and true labels.\n",
    "    -   **Assertions:**\n",
    "        -   Confirms that data is split into appropriate training and testing shapes.\n",
    "        -   Asserts that the model is fitted successfully.\n",
    "        -   Checks that predictions (`y_pred`) are NumPy arrays of the correct shape.\n",
    "        -   Verifies that the computed `metrics` dictionary contains all expected classification metrics (accuracy, precision, recall, f1_score, confusion_matrix).\n",
    "        -   Includes basic assertions on metric values (e.g., accuracy is between 0.4 and 1.0) to ensure the model is learning something reasonable, even with dummy data.\n",
    "        -   Prints informational messages to the console during the test execution, providing real-time feedback on the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/integration/test_model_metrics_integration.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    Integration test for the core model functionality: training, prediction,\n",
    "    and subsequent metric computation, now implemented using Python's\n",
    "    built-in `unittest` framework.\n",
    "    Verifies that the `ChurnPredictionModel` can be fitted and its predictions\n",
    "    are correctly processed by `compute_classification_metrics`.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_model_training_prediction_and_metrics_flow:\n",
    "        Creates a simple dummy dataset, simulates preprocessing, trains the model,\n",
    "        makes predictions, and asserts on the calculated metrics, ensuring the\n",
    "        entire flow works cohesively.\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "# Import components\n",
    "from src.model import ChurnPredictionModel, compute_classification_metrics\n",
    "from src.preprocessing import stratified_split # To get X, y, splits\n",
    "\n",
    "class TestModelMetricsIntegration(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create preprocessed dummy data directly in setUp\n",
    "        X = pd.DataFrame(np.random.rand(100, 5), columns=[f'feature_{i}' for i in range(5)])\n",
    "        y = pd.Series(np.random.randint(0, 2, 100))\n",
    "\n",
    "        # Apply a simple preprocessor to X, similar to how it would be done in the pipeline\n",
    "        preprocessor = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(preprocessor.fit_transform(X), columns=X.columns)\n",
    "\n",
    "        # Attach preprocessor and feature_columns as attributes, as `transform_features` would\n",
    "        X_scaled.attrs['preprocessor'] = preprocessor\n",
    "        X_scaled.attrs['feature_columns'] = list(X.columns)\n",
    "\n",
    "        self.X_full = X_scaled\n",
    "        self.y_full = y\n",
    "\n",
    "    def test_model_training_prediction_and_metrics_flow(self):\n",
    "        \"\"\"\n",
    "        Integration test: Verifies the flow from preprocessed data to model training,\n",
    "        prediction, and metric computation.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Integration Test: Model Training, Prediction, and Metrics Flow ---\")\n",
    "\n",
    "        # 1. Split data (simulating pipeline's splitting after preprocessing)\n",
    "        X_train, X_test, y_train, y_test = stratified_split(self.X_full, self.y_full, test_size=0.3, seed=42)\n",
    "        print(f\"Integration Test: Data split into train/test. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "        # 2. Instantiate and train model\n",
    "        # Use the preprocessor from the X_full's attributes (simulating transform_features)\n",
    "        model = ChurnPredictionModel(\n",
    "            classifier=LogisticRegression(random_state=42),\n",
    "            preprocessor=self.X_full.attrs['preprocessor'] # Pass the fitted preprocessor\n",
    "        )\n",
    "        fitted_model = model.fit(X_train, y_train)\n",
    "        self.assertIs(fitted_model, model)\n",
    "        print(\"Model fitted successfully.\")\n",
    "\n",
    "        # 3. Make predictions\n",
    "        y_pred = fitted_model.predict(X_test)\n",
    "        self.assertIsInstance(y_pred, np.ndarray)\n",
    "        self.assertEqual(y_pred.shape, y_test.shape)\n",
    "        print(\"Predictions made.\")\n",
    "\n",
    "        # 4. Compute metrics\n",
    "        metrics = compute_classification_metrics(y_test.to_numpy(), y_pred)\n",
    "        self.assertIn('accuracy', metrics)\n",
    "        self.assertIn('precision', metrics)\n",
    "        self.assertIn('recall', metrics)\n",
    "        self.assertIn('f1_score', metrics)\n",
    "        self.assertIn('confusion_matrix', metrics)\n",
    "        print(f\"Metrics computed: {metrics}\")\n",
    "\n",
    "        # Basic assertion on metric values (expect them to be reasonable, not perfect)\n",
    "        self.assertGreaterEqual(metrics['accuracy'], 0.4) # Should be better than random guess\n",
    "        self.assertLessEqual(metrics['accuracy'], 1.0) # Should not exceed 1.0\n",
    "\n",
    "        print(\"--- Integration Test Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_functional_test_description",
   "metadata": {},
   "source": [
    "# Functional test:\n",
    "\n",
    "## Main Functional\n",
    "\n",
    "This script provides **functional tests** for the `main.py` application entry point. Unlike unit tests that isolate individual functions, functional tests verify that the `main` function, which orchestrates the entire ML pipeline, operates correctly as a whole. This includes ensuring it calls the pipeline with the right parameters, handles its outputs (like success messages), and gracefully manages exceptions. These tests provide a higher level of confidence in the application's top-level behavior.\n",
    "\n",
    "-   **`TestMainFunctional` Class:**\n",
    "    -   **Objective:** Contains functional tests for the `main` function, focusing on its orchestration capabilities and error handling.\n",
    "    -   **Decorators (`@patch`):**\n",
    "        -   `@patch('main.run_churn_pipeline')`: Mocks the `run_churn_pipeline` function. This is crucial for controlling the pipeline's behavior (e.g., simulating success or failure) without executing the actual, complex ML pipeline code, allowing the test to focus solely on `main`'s orchestration logic.\n",
    "        -   `@patch('os.path.abspath', return_value='/mock/project/path/main.py')` and `@patch('os.path.dirname', return_value='/mock/project/path')`: These mocks simulate a consistent project root directory. This ensures that `main` constructs file paths correctly, regardless of where the test is actually run, making tests more reliable.\n",
    "\n",
    "-   **`test_main_orchestrates_pipeline_and_reports_success`:**\n",
    "    -   **Objective:** Verifies that `main` correctly triggers the `run_churn_pipeline` with the expected parameters and reports successful completion to standard output.\n",
    "    -   **Arrange:**\n",
    "        -   `dummy_model`, `dummy_metrics`: `MagicMock` objects are set as the `return_value` of `mock_run_churn_pipeline` to simulate a successful pipeline run with specific metrics.\n",
    "        -   `patch('sys.stdout', new_callable=io.StringIO)` and `patch('sys.stderr', new_callable=io.StringIO)`: These are used as context managers to capture any output (print statements, error messages) generated by `main` to `sys.stdout` and `sys.stderr` respectively. This allows for assertions on console output.\n",
    "    -   **Act:** `main()` is called without any arguments, simulating its typical invocation.\n",
    "    -   **Assert:**\n",
    "        -   `mock_run_churn_pipeline.assert_called_once()`: Confirms the pipeline function was called exactly once.\n",
    "        -   Assertions on `kwargs` (`mock_run_churn_pipeline.call_args`): Verifies that the `data_file_path`, `model_dir_path`, `target_column`, and `test_size` passed to `run_churn_pipeline` match the expected values derived from mocked paths and configuration.\n",
    "        -   Assertions on `stdout_output` and `stderr_output`: Checks that `main` printed the correct \"Starting...\", \"completed successfully!\", and \"Final Model Accuracy\" messages to `stdout` and that no errors were printed to `stderr`.\n",
    "\n",
    "-   **`test_main_handles_pipeline_exceptions_gracefully`:**\n",
    "    -   **Objective:** Tests `main`'s error handling, ensuring it gracefully catches exceptions from the pipeline, prints an informative error message, and then re-raises the exception for proper propagation.\n",
    "    -   **Arrange:**\n",
    "        -   `mock_run_churn_pipeline.side_effect = ValueError(error_message)`: Configures the mocked pipeline to raise a `ValueError`, simulating a pipeline failure.\n",
    "        -   `patch('sys.stdout', ...)` and `patch('sys.stderr', ...)`: Again, capture `stdout` and `stderr` to inspect console output.\n",
    "    -   **Act & Assert:**\n",
    "        -   `with self.assertRaisesRegex(ValueError, error_message): main()`: This block asserts that calling `main()` results in the expected `ValueError` being re-raised, confirming `main` does not suppress critical errors.\n",
    "        -   Assertions on `stderr_output`: Verifies that `main` printed an \"ERROR: Pipeline failed with exception...\" message to `stderr`.\n",
    "        -   Assertions on `stdout_output`: Confirms that the success message was *not* printed to `stdout` in case of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae90bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/test_main_functional.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    Functional tests for the main application entry point (`main.py`),\n",
    "    implemented using Python's built-in `unittest` framework.\n",
    "    These tests verify that the `main` function correctly orchestrates the\n",
    "    ML pipeline, handles its outputs, and manages exceptions gracefully.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_main_orchestrates_pipeline_and_reports_success:\n",
    "        Verifies that `main` calls the pipeline with correctly constructed\n",
    "        paths and prints a success message using the pipeline's results.\n",
    "    - test_main_handles_pipeline_exceptions_gracefully:\n",
    "        Verifies that `main` catches exceptions from the pipeline, prints\n",
    "        an error message to stderr, and re-raises the exception.\n",
    "\"\"\"\n",
    "import unittest\n",
    "from unittest.mock import patch, MagicMock\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import io # For capturing stdout/stderr\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src and main\n",
    "# Assumes test_main_functional.py is in 'ml_project/tests/'\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "# Import the main function to be tested\n",
    "from main import main\n",
    "from src.config import (\n",
    "    TARGET_COLUMN, TEST_SIZE, DATA_DIR_NAME, RAW_DATA_DIR_NAME,\n",
    "    DATASET_FILENAME, MODEL_STORE_DIR\n",
    ")\n",
    "\n",
    "\n",
    "class TestMainFunctional(unittest.TestCase):\n",
    "\n",
    "    # These patches are applied to each test method in this class.\n",
    "    # The 'return_value' for os.path mocks helps simulate a consistent project root.\n",
    "    @patch('main.run_churn_pipeline')\n",
    "    @patch('os.path.abspath', return_value='/mock/project/path/main.py')\n",
    "    @patch('os.path.dirname', return_value='/mock/project/path')\n",
    "    def test_main_orchestrates_pipeline_and_reports_success(\n",
    "        self,\n",
    "        mock_dirname, # Mocks from decorators are passed as arguments\n",
    "        mock_abspath,\n",
    "        mock_run_churn_pipeline\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Functional test to verify `main` correctly orchestrates the pipeline\n",
    "        and reports success based on the pipeline's output.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Functional Test: Main orchestrates pipeline successfully ---\")\n",
    "        # --- Arrange ---\n",
    "        # Define the mock return value for the entire pipeline run\n",
    "        dummy_model = MagicMock()\n",
    "        dummy_metrics = {\"accuracy\": 0.95123}\n",
    "        mock_run_churn_pipeline.return_value = (dummy_model, dummy_metrics)\n",
    "\n",
    "        # Use patch as a context manager to capture stdout/stderr cleanly\n",
    "        with patch('sys.stdout', new_callable=io.StringIO) as mock_stdout, \\\n",
    "             patch('sys.stderr', new_callable=io.StringIO) as mock_stderr:\n",
    "            # --- Act ---\n",
    "            main()\n",
    "\n",
    "            # --- Assert ---\n",
    "            # 1. Verify that the pipeline was called once with correctly constructed paths\n",
    "            mock_run_churn_pipeline.assert_called_once()\n",
    "\n",
    "            # Inspect the keyword arguments of the call\n",
    "            _, kwargs = mock_run_churn_pipeline.call_args\n",
    "            expected_data_path = str(Path('/mock/project/path') / DATA_DIR_NAME / RAW_DATA_DIR_NAME / DATASET_FILENAME)\n",
    "            expected_model_dir = str(Path('/mock/project/path') / MODEL_STORE_DIR)\n",
    "\n",
    "            self.assertEqual(kwargs['data_file_path'], expected_data_path)\n",
    "            self.assertEqual(kwargs['model_dir_path'], expected_model_dir)\n",
    "            self.assertEqual(kwargs['target_column'], TARGET_COLUMN)\n",
    "            self.assertEqual(kwargs['test_size'], TEST_SIZE)\n",
    "            print(\"Pipeline called with correct arguments.\")\n",
    "\n",
    "            # 2. Verify console output for success messages\n",
    "            stdout_output = mock_stdout.getvalue()\n",
    "            stderr_output = mock_stderr.getvalue()\n",
    "\n",
    "            self.assertIn(\"Starting Customer Churn Prediction Pipeline...\", stdout_output)\n",
    "            self.assertIn(\"Pipeline completed successfully!\", stdout_output)\n",
    "            self.assertIn(f\"Final Model Accuracy: {dummy_metrics['accuracy']:.4f}\", stdout_output)\n",
    "            self.assertEqual(stderr_output, \"\") # Ensure no errors were printed to stderr\n",
    "            print(\"Console output verified for success messages.\")\n",
    "        print(\"--- Functional Test Complete ---\")\n",
    "\n",
    "\n",
    "    @patch('main.run_churn_pipeline')\n",
    "    @patch('os.path.abspath', return_value='/mock/project/path/main.py')\n",
    "    @patch('os.path.dirname', return_value='/mock/project/path')\n",
    "    def test_main_handles_pipeline_exceptions_gracefully(\n",
    "        self,\n",
    "        mock_dirname,\n",
    "        mock_abspath,\n",
    "        mock_run_churn_pipeline\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Functional test to verify `main` catches exceptions from the pipeline,\n",
    "        logs an error, and re-raises the exception.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Functional Test: Main handles pipeline exceptions gracefully ---\")\n",
    "        # --- Arrange ---\n",
    "        # Configure the mock pipeline to raise an exception\n",
    "        error_message = \"Something went wrong during training\"\n",
    "        mock_run_churn_pipeline.side_effect = ValueError(error_message)\n",
    "\n",
    "        # Use patch as a context manager to capture stdout/stderr cleanly\n",
    "        with patch('sys.stdout', new_callable=io.StringIO) as mock_stdout, \\\n",
    "             patch('sys.stderr', new_callable=io.StringIO) as mock_stderr:\n",
    "\n",
    "            # --- Act & Assert ---\n",
    "            # Verify that the exception is caught and then re-raised by main\n",
    "            with self.assertRaisesRegex(ValueError, error_message):\n",
    "                main()\n",
    "\n",
    "            # Verify that an error message was printed to stderr\n",
    "            stdout_output = mock_stdout.getvalue()\n",
    "            stderr_output = mock_stderr.getvalue()\n",
    "\n",
    "            self.assertIn(f\"ERROR: Pipeline failed with exception: {error_message}\", stderr_output)\n",
    "            self.assertNotIn(\"Pipeline completed successfully!\", stdout_output)\n",
    "            print(\"Error message printed to stderr.\")\n",
    "            print(\"Exception correctly re-raised.\")\n",
    "        print(\"--- Functional Test Complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e_pipeline_testing_description",
   "metadata": {},
   "source": [
    "# End-to-End (E2E) Pipeline Testing:\n",
    "\n",
    "This script contains an **End-to-End (E2E) test** for the entire machine learning application pipeline, orchestrated by the `main.py` entry point. It's designed to mimic a real-world deployment scenario as closely as possible, providing the highest level of confidence that your entire system works as a cohesive unit. This test verifies the full ML pipeline from data loading and processing to model saving and log creation, using a real (albeit small) dataset.\n",
    "\n",
    "-   **`TestE2EPipeline` Class:**\n",
    "    -   **Objective:** Provides the E2E test for the complete ML pipeline, ensuring all integrated components function together correctly.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Prepares a temporary, isolated environment that simulates the project root directory. This is critical for E2E tests, as it prevents interference with the actual file system and ensures tests are repeatable.\n",
    "        -   **Temporary Directory Creation:** `tempfile.mkdtemp()` creates a unique temporary directory (`self.tmp_project_root`).\n",
    "        -   **Data Structure Simulation:** It then creates the necessary subdirectories (`data/raw`) within this temporary root.\n",
    "        -   **Dummy Dataset Creation:** A small, valid CSV dataset (`churn_data.csv`) is created and written into the simulated raw data directory, matching the expected input format for the pipeline. This ensures the pipeline processes realistic data.\n",
    "\n",
    "-   **`tearDown(self)` method:**\n",
    "    -   **Purpose:** Cleans up the temporary directory created in `setUp` after each test has completed. This ensures that tests are independent and do not leave behind any residual files or directories, maintaining a clean testing environment.\n",
    "\n",
    "-   **`test_main_pipeline_end_to_end`:**\n",
    "    -   **Objective:** This is the core E2E test. It runs the entire `main` function (the application's entry point) and then asserts on the final outputs, verifying the full system's behavior.\n",
    "    -   **Execution:** `main(output_base_dir=self.tmp_project_root)` is called. Critically, the temporary project root is passed as `output_base_dir`, directing all pipeline outputs (models, logs) to this isolated location. This ensures the test is self-contained.\n",
    "    -   **Assertions:**\n",
    "        -   **File Existence:** It asserts that the `MODEL_FILENAME` (the trained model) and `LOG_FILENAME` (the run metrics log) are correctly created in the expected `model_store` directory within the temporary project root. This confirms the pipeline's output generation.\n",
    "        -   **Log Content Verification:** It loads the generated log file (`LOG_FILENAME`), verifies it's a list, contains at least one run, and that the first run's `metrics` dictionary contains essential keys like \"accuracy\", \"f1_score\", and \"confusion_matrix\".\n",
    "        -   **Metric Value Sanity Check:** It asserts that the \"accuracy\" and \"f1_score\" are valid floating-point numbers within the reasonable range of 0.0 to 1.0. This provides a basic sanity check on the model's performance without requiring specific absolute values (as the dummy data might not yield highly predictable results).\n",
    "        -   **Confusion Matrix Structure:** It checks that the \"confusion_matrix\" is a list of lists with the expected dimensions (2x2), indicating correct reporting of classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/e2e_test.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    End-to-end test for the main application pipeline (`main.py`),\n",
    "    implemented using Python's built-in `unittest` framework.\n",
    "    This test verifies the entire ML pipeline from data loading to model saving\n",
    "    and log creation, using a real (albeit tiny) dataset.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_main_pipeline_end_to_end:\n",
    "        - Creates a temporary directory structure simulating the project.\n",
    "        - Prepares a small dummy dataset within this structure.\n",
    "        - Calls the `main` function to run the full pipeline.\n",
    "        - Asserts that the model file and log file are correctly created\n",
    "          in the expected output directory.\n",
    "        - Verifies that the logged metrics are valid (e.g., accuracy, f1_score\n",
    "          are within reasonable bounds).\n",
    "\"\"\"\n",
    "import unittest\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import joblib # Used for loading/saving models, not directly tested but part of flow\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import tempfile # For creating temporary directories\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src and main\n",
    "# Assumes e2e_test.py is in 'ml_project/tests/'\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "from main import main\n",
    "from src.config import LOG_FILENAME, MODEL_FILENAME, DATASET_FILENAME, DATA_DIR_NAME, RAW_DATA_DIR_NAME\n",
    "\n",
    "\n",
    "class TestE2EPipeline(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        Prepare a temporary directory to simulate the project root\n",
    "        and create a tiny dataset in the expected structure:\n",
    "        tmp_project_root/data/raw/churn_data.csv\n",
    "        \"\"\"\n",
    "        # Create a temporary directory for the entire simulated project\n",
    "        self.tmp_project_root = Path(tempfile.mkdtemp())\n",
    "\n",
    "        # Create required data directory structure within the temporary root\n",
    "        self.raw_data_dir = self.tmp_project_root / DATA_DIR_NAME / RAW_DATA_DIR_NAME\n",
    "        self.raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create a tiny valid dataset\n",
    "        sample_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
    "7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No\n",
    "5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No\n",
    "3668-QPYAX,Male,0,No,No,2,Yes,No,Fiber optic,Yes,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,53.85,108.15,Yes\n",
    "9237-HQITU,Female,0,No,No,2,Yes,No,Fiber optic,No,No,No,No,No,No,Month-to-month,Yes,Electronic check,70.70,,No\n",
    "9305-CDHLH,Female,0,No,No,8,Yes,Yes,Fiber optic,No,No,Yes,No,Yes,Yes,Month-to-month,Yes,Electronic check,99.65,820.5,Yes\n",
    "\"\"\"\n",
    "        # Use the filename from the config to ensure consistency with the pipeline\n",
    "        self.sample_file_path = self.raw_data_dir / DATASET_FILENAME\n",
    "        self.sample_file_path.write_text(sample_data)\n",
    "\n",
    "        print(f\"\\n--- E2E Test: Temporary project root set up at: {self.tmp_project_root} ---\")\n",
    "        print(f\"Sample data created at: {self.sample_file_path}\")\n",
    "\n",
    "\n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up the temporary directory and restore original working directory.\"\"\"\n",
    "        if os.path.exists(self.tmp_project_root):\n",
    "            shutil.rmtree(self.tmp_project_root)\n",
    "            print(f\"--- E2E Test: Cleaned up temporary directory: {self.tmp_project_root} ---\")\n",
    "\n",
    "    def test_main_pipeline_end_to_end(self):\n",
    "        \"\"\"\n",
    "        End-to-end test for the main training pipeline using a real dataset.\n",
    "        Verifies model and log file creation, and that metrics are valid.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Running End-to-End Pipeline Test ---\")\n",
    "\n",
    "        # Run the pipeline (main function)\n",
    "        # Pass the temporary directory as the output base to ensure files are saved there\n",
    "        main(output_base_dir=self.tmp_project_root)\n",
    "\n",
    "        # Expected output file paths (relative to self.tmp_project_root)\n",
    "        model_path = self.tmp_project_root / \"model_store\" / MODEL_FILENAME\n",
    "        log_path = self.tmp_project_root / \"model_store\" / LOG_FILENAME\n",
    "\n",
    "        # Assert files were created\n",
    "        self.assertTrue(model_path.exists(), f\"Model file was not saved at {model_path}\")\n",
    "        self.assertTrue(log_path.exists(), f\"Log file was not saved at {log_path}\")\n",
    "        print(\"Model and log files confirmed to exist.\")\n",
    "\n",
    "        # Load and verify log content\n",
    "        with open(log_path, \"r\") as f:\n",
    "            metrics_log = json.load(f)\n",
    "            # The log file contains a list of run details, each with a 'metrics' key\n",
    "            self.assertIsInstance(metrics_log, list, \"Log file content should be a list\")\n",
    "            self.assertGreater(len(metrics_log), 0, \"Log file should contain at least one run detail\")\n",
    "            \n",
    "            first_run_metrics = metrics_log[0].get(\"metrics\")\n",
    "            self.assertIsNotNone(first_run_metrics, \"First run detail should contain 'metrics' key\")\n",
    "\n",
    "            # Assert specific metrics are present and have reasonable values\n",
    "            self.assertIn(\"accuracy\", first_run_metrics)\n",
    "            self.assertIsInstance(first_run_metrics[\"accuracy\"], float)\n",
    "            self.assertGreaterEqual(first_run_metrics[\"accuracy\"], 0.0)\n",
    "            self.assertLessEqual(first_run_metrics[\"accuracy\"], 1.0)\n",
    "            print(\"Logged metrics verified (accuracy range).\")\n",
    "\n",
    "            self.assertIn(\"f1_score\", first_run_metrics)\n",
    "            self.assertIsInstance(first_run_metrics[\"f1_score\"], float)\n",
    "            self.assertGreaterEqual(first_run_metrics[\"f1_score\"], 0.0)\n",
    "            self.assertLessEqual(first_run_metrics[\"f1_score\"], 1.0)\n",
    "            print(\"Logged metrics verified (f1_score range).\")\n",
    "\n",
    "            self.assertIn(\"confusion_matrix\", first_run_metrics)\n",
    "            self.assertIsInstance(first_run_metrics[\"confusion_matrix\"], list)\n",
    "            self.assertEqual(len(first_run_metrics[\"confusion_matrix\"]), 2)\n",
    "            self.assertEqual(len(first_run_metrics[\"confusion_matrix\"][0]), 2)\n",
    "            print(\"Logged metrics verified (confusion matrix).\")\n",
    "\n",
    "        print(\"--- End-to-End Pipeline Test Complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c9aeceb7",
   "metadata": {},
   "source": [
    "# Deploying CI/CD pipeline in GitHub\n",
    "\n",
    "To deploy CI/DC on GitHub the following configurations are needed:\n",
    "\n",
    "- Establish a GitHub repository\n",
    "- Create Your Workflow File: Create a new directory at the root of your repository called `.github/workflows`. Inside this directory, create a new `YAML` file. The name of this file will be the name of your workflow (e.g., `ml_pipeline.yml`, `model_deploy.yml`). This file is where you define the entire CI/CD process. E.g.,\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- Define Workflow Triggers (`on`): Within your YAML file, the first thing to configure is when the workflow should run. Example:\n",
    "    - on: `push`: The workflow runs every time code is pushed to the repository (you can specify branches, e.g., branches: [main]).\n",
    "    - on: `pull_request`: The workflow runs when a pull request is opened, synchronised, or re-opened.\n",
    "    - on: `schedule`: You can set it to run at specific intervals, for instance, daily at a certain time to retrain a model.\n",
    "    - on: `workflow_dispatch`: Allows for manual triggering from the \"Actions\" tab.\n",
    "\n",
    "- Define Jobs (`jobs`): Workflows are made up of one or more \"jobs.\" Each job runs on a separate virtual machine (called a \"runner\"). You'll define distinct jobs for different phases of your pipeline, e.g., `data_preprocessing`, `model_training`, `model_testing`, `model_deployment`. You specify the operating system for the runner, e.g., `runs-on: ubuntu-latest`.\n",
    "\n",
    "- Define Steps within Jobs (`steps`): Each job consists of a sequence of \"steps.\" These steps are individual tasks that are executed in order. Common Steps for ML CI/CD:\n",
    "\n",
    "    - Checkout Code: Use the `actions/checkout@v4` Action to get your repository's code onto the runner.\n",
    "\n",
    "    - Set up Environment: Use `actions/setup-python@v5` to configure the Python version required.\n",
    "\n",
    "    - Install Dependencies: Run commands like `pip install -r requirements.txt` to install necessary libraries.\n",
    "\n",
    "    - Run Scripts: Execute your Python scripts for data loading, feature engineering, model training, and evaluation (e.g., `python src/train_model.py`).\n",
    "\n",
    "    - Run Tests: Execute unit tests, integration tests, and model performance tests (e.g., `pytest`).\n",
    "\n",
    "    - Build/Push Docker Images: If you're containerising your model.\n",
    "\n",
    "\n",
    "- Monitor Your Pipeline: Once configured, commit your YAML file to the repository. The workflow will trigger based on your defined `on` events. Go to the \"Actions\" tab in your GitHub repository. Here, you will see a list of all your workflow runs, their status (success, failure, in progress), and detailed logs for each step. This is your primary dashboard for observing the CI/CD process.\n",
    "\n",
    "Here below an example of the YAML file for our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaba58d",
   "metadata": {},
   "source": [
    "```\n",
    "name: Churn ML Project CI/CD (Unittest)\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "    paths:\n",
    "      - '02 Building & Integrating ML Pipelines/churn_ml_project_unittest/**'\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "    paths:\n",
    "      - '02 Building & Integrating ML Pipelines/churn_ml_project_unittest/**'\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    name: Run Unit, Integration, Functional, and E2E Tests\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    defaults:\n",
    "      run:\n",
    "        # Set the working directory to the project root within the repository\n",
    "        working-directory: \"02 Building & Integrating ML Pipelines/churn_ml_project_unittest\"\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout Repository\n",
    "      uses: actions/checkout@v3\n",
    "\n",
    "    - name: Set up Python 3.11\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.11'\n",
    "\n",
    "    - name: Install Dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "    - name: Run Unit Tests\n",
    "      run: |\n",
    "        # Run unit tests\n",
    "        # Assumes pure unit tests are in 'tests/unit/'\n",
    "        python -m unittest discover tests/unit -p \"test_*.py\"\n",
    "\n",
    "    - name: Run Integration Tests\n",
    "      run: |\n",
    "        # Run integration tests\n",
    "        # Assumes integration tests are in 'tests/integration/' and end with '_integration.py'\n",
    "        python -m unittest discover tests/integration -p \"test_*.py\"\n",
    "\n",
    "    - name: Run Functional Tests\n",
    "      run: |\n",
    "        # Run functional tests\n",
    "        # Assumes functional tests are in 'tests/functional/'\n",
    "        python -m unittest discover tests/functional -p \"test_*.py\"\n",
    "\n",
    "    - name: Run E2E Tests\n",
    "      run: |\n",
    "        # Run E2E tests\n",
    "        # Assumes E2E tests are in 'tests/e2e/' and are named e.g., 'e2e_test.py'\n",
    "        python -m unittest discover tests/e2e -p \"test_*.py\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485db47",
   "metadata": {},
   "source": [
    "Here an example of the requirements.txt for this project:\n",
    "\n",
    "```\n",
    "# Core ML Libraries\n",
    "pandas~=2.0.0      # For data manipulation and analysis\n",
    "numpy~=1.24.0      # For numerical operations\n",
    "scikit-learn~=1.2.0 # For machine learning models, preprocessing, and metrics\n",
    "joblib~=1.2.0      # For saving and loading Python objects, especially scikit-learn models\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875f479",
   "metadata": {},
   "source": [
    "### Pre-Push CI/CD Checklist\n",
    "\n",
    "Here's a checklist to run through before pushing your changes to the main branch, especially when that push is set to trigger your CI/CD pipeline on GitHub. This helps catch common issues before the automation kicks in.\n",
    "\n",
    "- [ ] Code Clean: All local changes committed, no temporary or debug code.\n",
    "\n",
    "- [ ] Tests Pass: All relevant local tests run successfully.\n",
    "\n",
    "- [ ] Dependencies Current: requirements.txt updated and tested if needed.\n",
    "\n",
    "- [ ] Documentation Updated: README.md reflects any changes in usage or setup.\n",
    "\n",
    "- [ ] Workflow Valid: CI/CD workflow files (.yml) are syntactically correct if modified.\n",
    "\n",
    "- [ ] Clear Commit: Commit message concisely describes the changes.\n",
    "\n",
    "- [ ] Final Review: Briefly checked all changes and confirming push to main.\n",
    "\n",
    "\n",
    "After your checklist is complete:\n",
    "\n",
    "- Prepare Changes: \n",
    "\n",
    "    - `git add .` (stages all your modified files)\n",
    "\n",
    "    - `git commit -m \"Your concise commit message\"` (saves the changes locally with a note)\n",
    "\n",
    "- Send to GitHub:\n",
    "\n",
    "    - `git push origin main` (sends your committed changes to the main branch on GitHub)\n",
    "\n",
    "- On GitHub, immediately after your push:\n",
    "\n",
    "    - Your updated code appears in the repository.\n",
    "\n",
    "    - GitHub Actions detects the change.\n",
    "\n",
    "    - Your CI/CD workflow automatically starts running.\n",
    "\n",
    "    - You can watch its progress and results in the \"Actions\" tab of your GitHub repository.\n",
    "\n",
    "\n",
    "If no errors are detected... \n",
    "\n",
    "__Congratulations, you have deployed an ML CI/CD Process!__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c626d",
   "metadata": {},
   "source": [
    "## README.md\n",
    "\n",
    "Serves as the front door to your project. It should be clear, comprehensive, and easy to navigate, allowing anyone (from a new collaborator to a future self) to quickly understand and engage with your work. `A good README.md` is like a clear project instruction manual and storefront. It should quickly tell you:\n",
    "\n",
    "- What it is: A brief project summary and its purpose.\n",
    "\n",
    "- How to set it up: Clear steps to get the code running locally, including dependencies (requirements.txt).\n",
    "\n",
    "- How to use it: Examples and commands to run the main functionalities.\n",
    "\n",
    "- What's inside (for ML): Basic info on the data and model, and key results.\n",
    "\n",
    "- Who made it & how to contribute: Information for collaborators and contact.\n",
    "\n",
    "- Legal bits: The project's license.\n",
    "\n",
    "- Essentially, it answers \"What is this?\", \"How do I get it working?\", and \"How do I use it?\".\n",
    "\n",
    "\n",
    "You can find an example of this file at the end of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224f104",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction ML Pipeline with Comprehensive Testing\n",
    "\n",
    "This project demonstrates a complete, production-style machine learning pipeline for predicting customer churn. Its primary focus is to showcase best practices in MLOps and software engineering, particularly a robust and layered testing strategy using Python's built-in `unittest` framework.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal of this project is to train a classification model (Logistic Regression) on a customer churn dataset. The pipeline handles data loading, cleaning, feature engineering, model training, evaluation, and artifact logging.\n",
    "\n",
    "The key highlight is the comprehensive test suite, which is structured to reflect the \"Testing Pyramid,\" ensuring code quality, reliability, and maintainability.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The project is organized into a clear and modular structure:\n",
    "\n",
    "```\n",
    "churn_ml_project_unittest_training1/\n",
    "├── data/\n",
    "│   └── raw/\n",
    "│       └── churn_data.csv      # Raw input data\n",
    "├── model_store/                # Output directory for saved models and logs\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── config.py               # Centralized configuration for the pipeline\n",
    "│   ├── data_loader.py          # Module for loading data\n",
    "│   ├── model.py                # Model class wrapper and metric functions\n",
    "│   ├── preprocessing.py        # Data cleaning and feature engineering functions\n",
    "│   └── pipeline.py             # Main pipeline orchestration logic\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   ├── unit/                   # Unit tests for individual functions\n",
    "│   ├── integration/            # Integration tests for component interactions\n",
    "│   ├── functional/             # Functional tests for user-facing behavior\n",
    "│   └── e2e/                    # End-to-end test for the full pipeline run\n",
    "├── main.py                     # Main entrypoint to run the pipeline\n",
    "└── requirements.txt            # Project dependencies\n",
    "```\n",
    "\n",
    "## Key Concepts Demonstrated\n",
    "\n",
    "*   **Modular Code**: Each part of the ML pipeline (data loading, preprocessing, modeling) is separated into its own module for clarity and reusability.\n",
    "*   **Centralized Configuration**: Key parameters, file paths, and column names are managed in `src/config.py` for easy modification.\n",
    "*   **MLOps Best Practices**: The pipeline saves the trained model and logs key metrics and metadata from each run, which is crucial for experiment tracking and reproducibility.\n",
    "*   **Layered Testing Strategy**: The project implements a full spectrum of tests:\n",
    "    *   **Unit Tests**: Fast tests that verify the smallest pieces of code (individual functions) in isolation.\n",
    "    *   **Integration Tests**: Tests that ensure different components of the pipeline work together correctly (e.g., data loading and cleaning).\n",
    "    *   **Functional Tests**: Tests that verify the application's entrypoint (`main.py`) from a user's perspective, checking its orchestration logic and console output.\n",
    "    *   **End-to-End (E2E) Tests**: A full run of the pipeline on a small, real dataset to ensure the entire system works as a whole and produces the expected final artifacts.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "1.  **Clone the repository:**\n",
    "    ```bash\n",
    "    git clone <repository-url>\n",
    "    cd churn_ml_project_unittest_training1\n",
    "    ```\n",
    "\n",
    "2.  **Create and activate a virtual environment (recommended):**\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
    "    ```\n",
    "\n",
    "3.  **Install the dependencies:**\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "## How to Run\n",
    "\n",
    "### Running the ML Pipeline\n",
    "\n",
    "To execute the entire machine learning pipeline, run the `main.py` script from the project's root directory.\n",
    "\n",
    "```bash\n",
    "python main.py\n",
    "```\n",
    "\n",
    "This will:\n",
    "1.  Load the raw data from `data/raw/churn_data.csv`.\n",
    "2.  Execute the full cleaning, transformation, and training pipeline.\n",
    "3.  Save the trained model to `model_store/churn_model.joblib`.\n",
    "4.  Save the run metrics and metadata to `model_store/log.json`.\n",
    "\n",
    "### Running the Tests\n",
    "\n",
    "The project uses Python's `unittest` framework. You can run all tests using the `discover` command from the root directory.\n",
    "\n",
    "**Run all tests:**\n",
    "```bash\n",
    "python -m unittest discover -s tests -v\n",
    "```\n",
    "\n",
    "**Run a specific type of test:**\n",
    "\n",
    "*   **Unit Tests:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/unit -v\n",
    "    ```\n",
    "*   **Integration Tests:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/integration -v\n",
    "    ```\n",
    "*   **Functional Tests:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/functional -v\n",
    "    ```\n",
    "*   **End-to-End Test:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/e2e -v\n",
    "    ```\n",
    "\n",
    "**Run a specific test file:**\n",
    "```bash\n",
    "python -m unittest tests/unit/test_model.py -v\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This project serves as a robust template for building and validating production-ready machine learning applications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae0f81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "752dd116",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf593914",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
